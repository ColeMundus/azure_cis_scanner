{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widen the display to fit your screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run it all from the commandline (in the docker container) with the following:\n",
    "base_dir is where the output scans directory will go.  -t 500 is so that it doesn't timeout.\n",
    "run_jnb -a '{\"subscription_id\": \"510f92e0-xxxx-yyyy-zzzz-095d37e6a299\", \"base_dir\": \"/engagements/cis_test\"}' -v  azure_cis_scanner.ipynb -t 500\n",
    "\n",
    "to run across multiple subscription_id's do\n",
    "for subscription in `cat /engagements/cis_test/scans/accounts.json | jq '.[].id'`; \\\n",
    "   do run_jnb -a '{\"subscription_id\": $subscription, \"base_dir\": \"/engagements/cis_test\"}' -v  azure_cis_scanner.ipynb -t 500; \\\n",
    "   done\n",
    "\n",
    "For more on run_jnb see https://github.com/hz-inova/run_jnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify that you have azure cli installed\n",
    "!az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if not, follow the instructions to install\n",
    "https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that yor are connected to the correct account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to azure.  This may require doing from the laptop/vm commandline or exec into the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your subscription_id here or run from the commandline\n",
    "subscription_id = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account set --subscription {subscription_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = !az account show\n",
    "account = yaml.load(account.nlstr)\n",
    "account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = account['id']\n",
    "subscription_name = account['name']\n",
    "subscription_dirname = subscription_name.split(' ')[0] + '-' + subscription_id.split('-')[0]\n",
    "subscription_dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/engagements/cis_test/'\n",
    "scanner_dir = '/praetorian-tools/azure_cis_scanner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write to disk or load as needed with %%writefile or %load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile /praetorian-tools/azure_cis_scanner/scanner/utils.py\n",
    "# %load /praetorian-tools/azure_cis_scanner/scanner/utils.py\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "import functools\n",
    "import json\n",
    "import requests\n",
    "\n",
    "token_expiry = None\n",
    "access_token = None\n",
    "filtered_data_dir = ''\n",
    "scan_data_dir = ''\n",
    "raw_data_dir = ''\n",
    "\n",
    "def set_data_paths(subscription_dirname, base_dir='.'):\n",
    "    \"\"\"\n",
    "    Given a base_dir, create subdirs scans/{day}/raw\n",
    "                                          /filtered\n",
    "    @returns: scan_data_dir, raw_data_dir\n",
    "    \"\"\"\n",
    "    # Get day in YYYY-MM-DD format\n",
    "\n",
    "    day = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    scan_data_dir = os.path.join(base_dir, 'scans', subscription_dirname, day)\n",
    "    print(\"scan_data_dir\", scan_data_dir)\n",
    "    raw_data_dir = scan_data_dir + '/raw'\n",
    "    print(\"raw_data_dir\", raw_data_dir)\n",
    "    if not os.path.exists(raw_data_dir):\n",
    "        os.makedirs(raw_data_dir)\n",
    "    filtered_data_dir = scan_data_dir + '/filtered'\n",
    "    print(\"filtered_data_dir\", filtered_data_dir)\n",
    "    if not os.path.exists(filtered_data_dir):\n",
    "        os.makedirs(filtered_data_dir)\n",
    "    return scan_data_dir, raw_data_dir, filtered_data_dir   \n",
    "\n",
    "def call(command, retrieving_access_token=False):\n",
    "    if not valid_token() and not retrieving_access_token:\n",
    "        get_access_token()\n",
    "    if(isinstance(command, str)) :\n",
    "        command = command.split()           # subprocess needs an array of arguments\n",
    "    try :\n",
    "        print('running: ', command)\n",
    "        return subprocess.check_output(command, shell=False, stderr=subprocess.STDOUT).decode('utf-8')\n",
    "    except:\n",
    "        print(\"An exception occurred while processing command \" + str(command) + \" Halting execution!\")\n",
    "        sys.exit()\n",
    "\n",
    "def verify_subscription_id_format(subscriptionId) :\n",
    "    r = re.compile(\"([a-f]|[0-9]){8}-([a-f]|[0-9]){4}-([a-f]|[0-9]){4}-([a-f]|[0-9]){4}-([a-f]|[0-9]){12}\")\n",
    "    if r.match(subscriptionId):\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "def valid_token():\n",
    "     if (not token_expiry) or (datetime.datetime.utcnow() > token_expiry):\n",
    "        return False\n",
    "     else:\n",
    "        return True\n",
    "    \n",
    "def get_subscription_id() :\n",
    "    current_context = jsonify(call(\"az account show\"))\n",
    "    return current_context[\"id\"]\n",
    "\n",
    "def get_access_token():\n",
    "    global token_expiry, access_token\n",
    "    if not valid_token():\n",
    "        complete_token = call(\"az account get-access-token\", retrieving_access_token=True)\n",
    "        complete_token = jsonify(complete_token)\n",
    "        access_token = complete_token[\"accessToken\"]\n",
    "        token_expiry = complete_token[\"expiresOn\"]\n",
    "        print(token_expiry)\n",
    "        token_expiry = datetime.datetime.strptime(token_expiry, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    return access_token\n",
    "\n",
    "def make_request(url, args=[]):\n",
    "    print('requesting ', url)\n",
    "    authorization_headers = {\"Authorization\" : \"Bearer \" + get_access_token()}\n",
    "    r = requests.get(url, headers=authorization_headers)\n",
    "    return r.text\n",
    "\n",
    "def jsonify(jsonString) :\n",
    "    return json.loads(jsonString)\n",
    "\n",
    "def stringify(jsonObject) :\n",
    "    return json.dumps(jsonObject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_access_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run set_data_paths to create {base_dir}/phases/azure/scans/{date}/raw json files for raw output of commands\n",
    "Run the scripts for the selection criteria that determine failure to produce a filtered subset of the above writing to /phases/azure/scans/{date}/granular_findings_json for the json data of a finding\n",
    "Run a script to generate the grouped findings at least partially\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_expiry)\n",
    "print(datetime.datetime.utcnow())\n",
    "print(datetime.datetime.utcnow() > token_expiry)\n",
    "valid_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are running this notebook from a container with ~/engagements mounted into /engagements in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_data_dir, raw_data_dir, filtered_data_dir = set_data_paths(subscription_dirname, base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-ride dates if necessary\n",
    "# scan_data_dir = '/engagements/cis_test/scans/2018-05-26'\n",
    "# raw_data_dir = '/engagements/cis_test/scans/2018-05-26/raw'\n",
    "# filtered_data_dir = '/engagements/cis_test/scans/2018-05-26/filtered'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error like the following indicates you have not logged in correctly\n",
    "ScannerError: while scanning a simple key\n",
    "  in \"<unicode string>\", line 6, column 1:\n",
    "    100    97  100    97    0     0  ... \n",
    "    ^\n",
    "could not find expected ':'\n",
    "  in \"<unicode string>\", line 7, column 1:\n",
    "    {\"error\":{\"code\":\"SubscriptionNo ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_path = os.path.join(base_dir, 'scans', 'accounts.json')\n",
    "\n",
    "def get_accounts(accounts_path):\n",
    "    \"\"\"\n",
    "    @accounts_path: string - path to output json file\n",
    "    \"\"\"\n",
    "    accounts = !az account list\n",
    "    accounts = yaml.safe_load(accounts.nlstr)\n",
    "    with open(accounts_path, 'w') as f:\n",
    "        json.dump(accounts, f, indent=4, sort_keys=True)\n",
    "    return accounts\n",
    "\n",
    "def load_accounts(accounts_path):\n",
    "    with open(accounts_path, 'r') as f:\n",
    "        accounts = yaml.safe_load(f)\n",
    "    return accounts\n",
    "\n",
    "def get_resource_groups(resource_groups_path):\n",
    "    \"\"\"\n",
    "    @network_path: string - path to output json file\n",
    "    \"\"\"\n",
    "    resource_groups = !az group list\n",
    "    resource_groups = yaml.safe_load(resource_groups.nlstr)\n",
    "    with open(resource_groups_path, 'w') as f:\n",
    "        json.dump(resource_groups, f, indent=4, sort_keys=True)\n",
    "    return resource_groups\n",
    "\n",
    "def load_resource_groups(resource_groups_path):\n",
    "    with open(resource_groups_path, 'r') as f:\n",
    "        resource_groups = yaml.safe_load(f)\n",
    "    return resource_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accounts_path)\n",
    "get_accounts(accounts_path)\n",
    "resource_groups_path = os.path.join(raw_data_dir, \"resource_groups.json\")\n",
    "get_resource_groups(resource_groups_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile scanner_dir + '/scanner/security_center.py'\n",
    "# %load scanner_dir + '/scanner/security_center.py'\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "security_center_path = os.path.join(raw_data_dir, \"security_center.json\")\n",
    "security_center_filtered_path = os.path.join(filtered_data_dir, 'security_center_filtered.json')\n",
    "\n",
    "def get_security_center(security_center_path):\n",
    "    \"\"\"\n",
    "    Query Azure api for storage accounts info and save to disk\n",
    "    \"\"\"\n",
    "    output = !az account get-access-token --query \"{subscripton:subscription,accessToken:accessToken}\" --out tsv\n",
    "    print(output.nlstr.split())\n",
    "    subscription_id, token = output.nlstr.split()\n",
    "    security_center = !curl -X GET -H \"Authorization: Bearer {token}\" -H \"Content-Type: application/json\" https://management.azure.com/subscriptions/{subscription_id}/providers/microsoft.Security/policies?api-version=2015-06-01-preview 2>/dev/null\n",
    "    security_center = yaml.load(security_center.nlstr)\n",
    "    security_center = security_center['value']\n",
    "    print(security_center)\n",
    "        \n",
    "    with open(security_center_path, 'w') as f:\n",
    "        yaml.dump(security_center, f)\n",
    "    return security_center\n",
    "\n",
    "def load_security_center(security_center_filtered_path):\n",
    "    with open(security_center_path, 'r') as f:\n",
    "        security_center = yaml.load(f)\n",
    "    return security_center\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Generate json for the security_center findings\n",
    "    \"\"\"\n",
    "    get_security_center(security_center_path)\n",
    "\n",
    "def test_controls():\n",
    "    \"\"\"\n",
    "    Generate filtered (failing) output in json\n",
    "    \"\"\"\n",
    "    security_center = load_security_center(security_center_path)\n",
    "    security_center_results = {}\n",
    "\n",
    "    security_center_results['automatic_provisioning_of_monitoring_agent_is_set_to_on'] = automatic_provisioning_of_monitoring_agent_is_set_to_on_2_2(security_center)\n",
    "    security_center_results['system_updates_is_set_to_on'] = system_updates_is_set_to_on_2_3(security_center)\n",
    "    security_center_results['security_configurations_is_set_to_on'] = security_configurations_is_set_to_on_2_4(security_center)\n",
    "    security_center_results['endpoint_protection_is_set_to_on'] = endpoint_protection_is_set_to_on_2_5(security_center)\n",
    "    security_center_results['disk_encryption_is_set_to_on'] = disk_encryption_is_set_to_on_2_6(security_center)\n",
    "    security_center_results['network_security_groups_is_set_to_on'] = network_security_groups_is_set_to_on_2_7(security_center)\n",
    "    security_center_results['web_application_firewall_is_set_to_on'] = web_application_firewall_is_set_to_on_2_8(security_center)\n",
    "    security_center_results['next_generation_firewall_is_set_to_on'] = next_generation_firewall_is_set_to_on_2_9(security_center)\n",
    "    security_center_results['vulnerability_assessment_is_set_to_on'] = vulnerability_assessment_is_set_to_on_2_10(security_center)\n",
    "    security_center_results['storage_encryption_is_set_to_on'] = storage_encryption_is_set_to_on_2_11(security_center)\n",
    "    security_center_results['just_in_time_access_is_set_to_on'] = just_in_time_access_is_set_to_on_2_12(security_center)\n",
    "    security_center_results['adaptive_application_controls_is_set_to_on'] = adaptive_application_controls_is_set_to_on_2_13(security_center)\n",
    "    security_center_results['sql_auditing_and_threat_detection_is_set_to_on'] = sql_auditing_and_threat_detection_is_set_to_on_2_14(security_center)\n",
    "    security_center_results['sql_encryption_is_set_to_on'] = sql_encryption_is_set_to_on_2_15(security_center)\n",
    "    security_center_results['security_contact_emails_is_set'] = security_contact_emails_is_set_2_16(security_center)\n",
    "    security_center_results['security_contact_phone_number_is_set'] = security_contact_phone_number_is_set_2_17(security_center)\n",
    "    security_center_results['send_me_emails_about_alerts_is_set_to_on'] = send_me_emails_about_alerts_is_set_to_on_2_18(security_center)\n",
    "    security_center_results['send_email_also_to_subscription_owners_is_set_to_on'] = send_email_also_to_subscription_owners_is_set_to_on_2_19(security_center)\n",
    "                \n",
    "    with open(security_center_filtered_path, 'w') as f:\n",
    "        json.dump(security_center_results, f, indent=4, sort_keys=True)\n",
    "    return security_center_results\n",
    "\n",
    "def automatic_provisioning_of_monitoring_agent_is_set_to_on_2_2(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        automatic_provisioning_of_monitoring_agent = item['properties']['logCollection']\n",
    "        if automatic_provisioning_of_monitoring_agent != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"automatic_provisioning_of_monitoring_agent_is_set_to_on\",\n",
    "                \"negative_name\": \"automatic_provisioning_of_monitoring_agent_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "    \n",
    "def system_updates_is_set_to_on_2_3(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        system_updates = item['properties']['recommendations']['patch']\n",
    "        if system_updates != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"system_updates_is_set_to_on\",\n",
    "                \"negative_name\": \"system_updates_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}    \n",
    "\n",
    "def security_configurations_is_set_to_on_2_4(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        security_configurations = item['properties']['recommendations']['baseline']\n",
    "        if security_configurations != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "            \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"security_configurations_is_set_to_on\",\n",
    "                \"negative_name\": \"security_configurations_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata} \n",
    "\n",
    "def endpoint_protection_is_set_to_on_2_5(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        endpoint_protection = item['properties']['recommendations']['antimalware']\n",
    "        if endpoint_protection != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"endpoint_protection_is_set_to_on\",\n",
    "                \"negative_name\": \"endpoint_protection_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def disk_encryption_is_set_to_on_2_6(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        disk_encryption = item['properties']['recommendations']['diskEncryption']\n",
    "        if disk_encryption != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"disk_encryption_is_set_to_on\",\n",
    "                \"negative_name\": \"disk_encryption_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def network_security_groups_is_set_to_on_2_7(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        nsgs = item['properties']['recommendations']['nsgs']\n",
    "        if nsgs != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"network_security_groups_is_set_to_on\",\n",
    "                \"negative_name\": \"network_security_groups_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def web_application_firewall_is_set_to_on_2_8(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        waf = item['properties']['recommendations']['waf']\n",
    "        if waf != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"web_application_firewall_is_set_to_on\",\n",
    "                \"negative_name\": \"web_application_firewall_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def next_generation_firewall_is_set_to_on_2_9(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        ngfw = item['properties']['recommendations']['ngfw']\n",
    "        if ngfw != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"automatic_provisioning_of_monitoring_agent_is_set_to_on\",\n",
    "                \"negative_name\": \"automatic_provisioning_of_monitoring_agent_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def vulnerability_assessment_is_set_to_on_2_10(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        vulnerability_assessment = item['properties']['recommendations']['vulnerabilityAssessment']\n",
    "        if vulnerability_assessment != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "            \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"automatic_provisioning_of_monitoring_agent_is_set_to_on\",\n",
    "                \"negative_name\": \"automatic_provisioning_of_monitoring_agent_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def storage_encryption_is_set_to_on_2_11(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        storage_encryption = item['properties']['recommendations']['storageEncryption']\n",
    "        if storage_encryption != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"storage_encryption_is_set_to_on\",\n",
    "                \"negative_name\": \"storage_encryption_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def just_in_time_access_is_set_to_on_2_12(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        jit = item['properties']['recommendations']['jitNetworkAccess']\n",
    "        if jit != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"just_in_time_access_is_set_to_on\",\n",
    "                \"negative_name\": \"just_in_time_access_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def adaptive_application_controls_is_set_to_on_2_13(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        security_configurations = item['properties']['recommendations']['appWhitelisting']\n",
    "        if security_configurations != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"adaptive_application_controls_is_set_to_on\",\n",
    "                \"negative_name\": \"adaptive_application_controls_noto_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def sql_auditing_and_threat_detection_is_set_to_on_2_14(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        sqlAuditing = item['properties']['recommendations']['sqlAuditing']\n",
    "        if sqlAuditing != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"sql_auditing_and_threat_detection_is_set_to_on\",\n",
    "                \"negative_name\": \"sql_auditing_and_threat_detection_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def sql_encryption_is_set_to_on_2_15(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        sql_tde = item['properties']['recommendations']['sqlTde']\n",
    "        if sql_tde != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"sql_encryption_is_set_to_on\",\n",
    "                \"negative_name\": \"sql_encryption_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def security_contact_emails_is_set_2_16(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        emails = item['properties']['securityContactConfiguration']['securityContactEmails']\n",
    "        if not emails:\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"security_contact_emails_is_set\",\n",
    "                \"negative_name\": \"security_contact_emails_not_set\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def security_contact_phone_number_is_set_2_17(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        phone = item['properties']['securityContactConfiguration']['securityContactPhone']\n",
    "        if not phone:\n",
    "            items_flagged_list.append((resource_group))\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"security_contact_phone_number_is_set\",\n",
    "                \"negative_name\": \"security_contact_phone_number_not_set\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def send_me_emails_about_alerts_is_set_to_on_2_18(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        notifications = item['properties']['securityContactConfiguration']['areNotificationsOn']\n",
    "        if notifications != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"send_email_alerts_about_alerts_is_set_to_on\",\n",
    "                \"negative_name\": \"send_email_alerts_about_alerts_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def send_email_also_to_subscription_owners_is_set_to_on_2_19(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        send_admin = item['properties']['securityContactConfiguration']['sendToAdminOn']\n",
    "        if send_admin != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"send_email_also_to_subscription_owners_is_set_to_on\",\n",
    "                \"negative_name\": \"send_email_also_to_subscription_owners_is_set_to_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(security_center_filtered_path)\n",
    "get_data()\n",
    "security_center = load_security_center(security_center_path)\n",
    "security_center[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_logs_path = os.path.join(raw_data_dir, 'activity_logs.json')\n",
    "storage_accounts_path = os.path.join(raw_data_dir, 'storage_accounts.json')\n",
    "\n",
    "def get_storage_accounts(storage_accounts_path):\n",
    "    \"\"\"\n",
    "    Query Azure api for storage accounts info and save to disk\n",
    "    \"\"\"\n",
    "    storage_accounts = !az storage account list\n",
    "    storage_accounts = yaml.safe_load(storage_accounts.nlstr)\n",
    "        \n",
    "    with open(storage_accounts_path, 'w') as f:\n",
    "        json.dump(storage_accounts, f, indent=4, sort_keys=True)\n",
    "    return storage_accounts\n",
    "\n",
    "def load_storage_accounts(storage_accounts_path):\n",
    "    with open(storage_accounts_path, 'r') as f:\n",
    "        storage_accounts = yaml.safe_load(f)\n",
    "    return storage_accounts\n",
    "\n",
    "activity_logs_starttime_timedelta = datetime.timedelta(days=90)\n",
    "def get_start_time(timedelta=datetime.timedelta(days=90)):\n",
    "    \"\"\"\n",
    "    Given datetime.timedelta(days=days, hours=hours), return string in iso tz format \n",
    "    \"\"\"\n",
    "    return datetime.datetime.strftime(datetime.datetime.now() - timedelta, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def get_activity_logs(activity_logs_path, resource_groups):\n",
    "    activity_logs = {}\n",
    "    start_time = get_start_time(activity_logs_starttime_timedelta)\n",
    "    for resource_group in resource_groups:\n",
    "        resource_group = resource_group['name']\n",
    "        activity_log = !az monitor activity-log list --resource-group {resource_group} --start-time {start_time}\n",
    "        activity_log = yaml.safe_load(activity_log.nlstr)\n",
    "        activity_logs[resource_group] = activity_log\n",
    "    with open(activity_logs_path, 'w') as f:\n",
    "        json.dump(activity_logs, f, indent=4, sort_keys=True)\n",
    "    return activity_logs    \n",
    "\n",
    "def load_activity_logs(activity_logs_path):\n",
    "    with open(activity_logs_path, 'r') as f:\n",
    "        activity_logs = yaml.safe_load(f)\n",
    "    return activity_logs\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "# Tests\n",
    "#################\n",
    "\n",
    "def secure_transfer_required_is_set_to_enabled_3_1(storage_accounts):\n",
    "    items_flagged_list = []\n",
    "    for account in storage_accounts:\n",
    "        name = account['name']\n",
    "        resource_group = account['resourceGroup']\n",
    "        enabled = account['enableHttpsTrafficOnly']\n",
    "        if enabled != True:\n",
    "            items_flagged_list.append((resource_group, name))\n",
    "    stats = {'items_flagged': len(items_flagged_list), \"items_checked\": len(storage_accounts)}\n",
    "    metadata = {\"finding_name\": \"secure_transfer_required_is_set_to_enabled\",\n",
    "                \"negative_name\": \"secure_transfer_required_not_enabled\",\n",
    "                \"columns\": [\"Resource Group\", \"Storage Account Name\"]}\n",
    "    return {\"items\": items_flagged_list, \n",
    "            \"stats\": stats, \n",
    "            \"metadata\": metadata }\n",
    "            \n",
    "\n",
    "def storage_service_encryption_is_set_to_enabled_for_blob_service_3_2(storage_accounts):\n",
    "    items_flagged_list = []\n",
    "    for account in storage_accounts:\n",
    "        if account['encryption']['services']['blob'] and (account['encryption']['services']['blob']['enabled'] != True):\n",
    "            items_flagged_list.append((account['resourceGroup'], account['name']))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(storage_accounts)}\n",
    "    metadata = {\"finding_name\": \"storage_service_encryption_is_set_to_enabled_for_blob_service\",\n",
    "                \"negative_name\": \"storage_service_encryption_not_enabled_for_blob_service\",\n",
    "                \"columns\": [\"Resource Group\",\"Storage Account Name\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata }\n",
    "           \n",
    "\n",
    "# may need to run section 6 Networking first to get activity_log\n",
    "def storage_account_access_keys_are_periodically_regenerated_3_3(activity_logs, storage_accounts, resource_groups):\n",
    "    items_flagged_list = []\n",
    "    \n",
    "    max_rotation_days = 90\n",
    "    most_recent_rotations = {}\n",
    "    for resource_group in resource_groups:\n",
    "        resource_group_name = resource_group['name']\n",
    "        for log in activity_logs[resource_group_name]:\n",
    "            if log[\"authorization\"] and (log[\"authorization\"][\"action\"] == \"Microsoft.Storage/storageAccounts/regenerateKey/action\"):\n",
    "                scope = log[\"authorization\"][\"scope\"]\n",
    "                _, _, _, resource_group, _, _, _, storage_account_name = scope.split('/')\n",
    "                timestamp = log[\"eventTimestamp\"]\n",
    "                event_day = timestamp.split('T')[0]\n",
    "                event_day = datetime.datetime.strptime(event_time, '%Y-%m-%d')\n",
    "                status = log[\"status\"][\"localizedValue\"]\n",
    "                if status == \"Success\":\n",
    "                    # fromtimestamp(0) gives smallest date possible in epoch time\n",
    "                    existing_update = most_recent_rotations.get(storage_account, datetime.datetime.fromtimestamp(0))\n",
    "                    most_recent_rotations[storage_account] = max(existing_update, event_time)\n",
    "\n",
    "    for storage_account in storage_accounts:\n",
    "        resource_group = storage_account[\"resourceGroup\"]\n",
    "        storage_account_name = storage_account['name']\n",
    "        items_flagged_list.append((resource_group, storage_account_name, str(most_recent_rotations.get(storage_account_name, \"No rotation\"))))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(storage_accounts)}\n",
    "    metadata = {\"finding_name\": \"storage_account_access_keys_are_periodically_regenerated\",\n",
    "                \"negative_name\": \"storage_account_access_keys_not_periodically_regenerated\",\n",
    "                \"columns\": [\"Resource Group\", \"Storage Account\", \"Rotation Date\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def shared_access_signature_tokens_expire_within_an_hour_3_4(storage_accounts):\n",
    "    \"\"\"\n",
    "    There is no automation possible for this currently\n",
    "    Manual\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def shared_access_signature_tokens_are_allowed_only_over_https_3_5(storage_accounts):\n",
    "    \"\"\"\n",
    "    There is no automation possible for this currently\n",
    "    Manual\n",
    "    \"\"\"\n",
    "    pass\n",
    "                                      \n",
    "def storage_service_encryption_is_set_to_enabled_for_file_service_3_6(storage_accounts):\n",
    "    items_flagged_list = []\n",
    "    stats = {}\n",
    "    for account in storage_accounts:\n",
    "        if account['encryption']['services']['file'] and (account['encryption']['services']['file']['enabled'] != True):\n",
    "            items_flagged_list.append((account['name']))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(storage_accounts)}\n",
    "    metadata = {\"finding_name\": \"storage_service_encryption_is_set_to_enabled_for_file_service\",\n",
    "                \"negative_name\": \"storage_service_encryption_not_enabled_for_file_service\",\n",
    "                \"columns\": [\"Storage Account Name\"]}\n",
    "\n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def public_access_level_is_set_to_private_for_blob_containers_3_7(storage_accounts):\n",
    "    items_flagged_list = []\n",
    "    items_checked = 0\n",
    "    for account in storage_accounts:\n",
    "        account_name = account[\"name\"]\n",
    "        resource_group = account[\"resourceGroup\"]\n",
    "        # get a key that works.  likely this will be a specific key not key[0]\n",
    "        keys = !az storage account keys list --account-name {account_name} --resource-group {resource_group}\n",
    "        keys = yaml.safe_load(keys.nlstr)\n",
    "        key = keys[0]\n",
    "        container_list = !az storage container list --account-name {account_name} --account-key {account_key}\n",
    "        container_list = yaml.load(container_list.nlstr)\n",
    "        for container in container_list:\n",
    "            print(container)\n",
    "            items_checked += 1\n",
    "            public_access = container[\"properties\"][\"public_access\"]\n",
    "            if public_access == True:\n",
    "                items_flagged_list.append((account_name, container))\n",
    "    stats = {'items_flagged': len(items_flagged_list), \"items_checked\": items_checked}\n",
    "    metadata = {\"finding_name\": \"public_access_level_is_set_to_private_for_blob_containers\",\n",
    "                \"negative_name\": \"public_access_level_not_private_for_blob_containers\",\n",
    "                \"columns\": [\"Storage Account Name\", \"Container\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata }\n",
    "\n",
    "    \n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Generate json for the storage_accounts findings\n",
    "    \"\"\"\n",
    "    resource_groups = get_resource_groups(resource_groups_path)\n",
    "    get_activity_logs(activity_logs_path, resource_groups)\n",
    "    get_storage_accounts(storage_accounts_path)\n",
    "\n",
    "def test_controls():\n",
    "    \"\"\"\n",
    "    Generate filtered (failing) output in json\n",
    "    \"\"\"\n",
    "    resource_groups = load_resource_groups(resource_groups_path)\n",
    "    storage_accounts = load_storage_accounts(storage_accounts_path)\n",
    "    activity_logs = load_activity_logs(activity_logs_path)\n",
    "    \n",
    "    storage_results = {}\n",
    "    storage_results['secure_transfer_required_is_set_to_enabled'] = secure_transfer_required_is_set_to_enabled_3_1(storage_accounts)\n",
    "    storage_results['storage_service_encryption_is_set_to_enabled_for_blob_service'] = storage_service_encryption_is_set_to_enabled_for_blob_service_3_2(storage_accounts)\n",
    "    storage_results['storage_account_access_keys_are_periodically_regenerated'] = storage_account_access_keys_are_periodically_regenerated_3_3(activity_logs, storage_accounts, resource_groups)\n",
    "    storage_results['storage_service_encryption_is_set_to_enabled_for_file_service'] = storage_service_encryption_is_set_to_enabled_for_file_service_3_6(storage_accounts)\n",
    "    #storage_results['public_access_level_is_set_to_private_for_blob_containers'] = public_access_level_is_set_to_private_for_blob_containers_3_7(storage_accounts)\n",
    "        \n",
    "    with open(os.path.join(scan_data_dir, 'filtered', 'storage_accounts_filtered.json'), 'w') as f:\n",
    "        json.dump(storage_results, f, indent=4, sort_keys=True)\n",
    "    return storage_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(storage_accounts_path)\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_finding??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If editing utils.py you may need to reload here.\n",
    "# from importlib import reload\n",
    "# reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /praetorian-tools/azure_cis_scanner/scanner/sql_auditing.py\n",
    "# Generate files in azcli_out\n",
    "import utils\n",
    "import yaml\n",
    "\n",
    "sql_servers_path = os.path.join(raw_data_dir, 'sql_servers.json')\n",
    "sql_server_policies_path = os.path.join(raw_data_dir, 'sql_server_policies.json')\n",
    "\n",
    "def get_data():\n",
    "    sql_servers = get_sql_servers(sql_servers_path)\n",
    "    get_sql_server_policies(sql_server_policies_path, sql_servers)\n",
    "\n",
    "def get_sql_servers(sql_servers_path) :\n",
    "    sql_servers_string = utils.call(\"az sql server list\")\n",
    "    sql_servers_json = utils.jsonify(sql_servers_string)\n",
    "    with open(sql_servers_path, 'w') as f:\n",
    "        json.dump(sql_servers_json, f, indent=4, sort_keys=True)\n",
    "    return sql_servers_json\n",
    "\n",
    "def get_sql_server_policies(sql_server_policies_path, sql_servers):\n",
    "    results = {}\n",
    "    subscriptionId = utils.get_subscription_id()\n",
    "    for sql_server in sql_servers:\n",
    "        server_name = sql_server['name']\n",
    "        resource_group = sql_server['resourceGroup']\n",
    "        sql_server_policies = {}\n",
    "        sql_server_policies['audit_policy'] = get_sql_server_audit_policies(subscriptionId, resource_group, server_name)\n",
    "        sql_server_policies['threat_detection_policy'] = get_sql_server_threat_detection_policies(subscriptionId, resource_group, server_name)\n",
    "        sql_server_policies['active_directory_admin_configurations'] = get_sql_server_active_directory_admin_configuration(subscriptionId, resource_group, server_name)\n",
    "        results[(resource_group, server_name)] = sql_server_policies\n",
    "    with open(sql_server_policies_path, 'w') as f:\n",
    "        yaml.dump(results, f)\n",
    "    return results\n",
    "\n",
    "def load_sql_servers(sql_servers_path):\n",
    "    with open(sql_servers_path, 'r') as f:\n",
    "        sql_servers = yaml.load(f)\n",
    "    return sql_servers\n",
    "\n",
    "def load_sql_server_policies(sql_server_policies_path):\n",
    "    with open(sql_server_policies_path, 'r') as f:\n",
    "        sql_server_policies = yaml.load(f)\n",
    "    return sql_server_policies\n",
    "    \n",
    "# This function will be recentered around Azure Command Line, after such an option becomes available.\n",
    "def get_sql_server_audit_policies(subscriptionId, resource_group, server_name):\n",
    "    endpoint = \"https://management.azure.com/subscriptions/\"+subscriptionId+\"/resourceGroups/\"+resource_group+\"/providers/Microsoft.Sql/servers/\"+server_name+\"/auditingSettings/Default?api-version=2015-05-01-preview\"\n",
    "    sql_server_audit_policy = utils.make_request(endpoint)\n",
    "    sql_server_audit_policy = utils.jsonify(sql_server_audit_policy)\n",
    "    return sql_server_audit_policy\n",
    "\n",
    "# This function will be recentered around Azure Command Line, after such an option becomes available.\n",
    "def get_sql_server_threat_detection_policies(subscriptionId, resource_group, server_name):\n",
    "    endpoint = \"https://management.azure.com/subscriptions/\"+subscriptionId+\"/resourceGroups/\"+resource_group+\"/providers/Microsoft.Sql/servers/\"+server_name+\"/securityAlertPolicies/Default?api-version=2015-05-01-preview\"\n",
    "    sql_server_threat_detection_policy = utils.make_request(endpoint)\n",
    "    sql_server_threat_detection_policy = utils.jsonify(sql_server_threat_detection_policy)\n",
    "    return sql_server_threat_detection_policy\n",
    "\n",
    "def get_sql_server_active_directory_admin_configuration(subscriptionId, resource_group, server_name):\n",
    "    active_directory_admin_configuration = utils.call(\"az sql server ad-admin list --resource-group \" + resource_group + \" --server \" + server_name)\n",
    "    active_directory_admin_configuration = utils.jsonify(active_directory_admin_configuration)\n",
    "    return active_directory_admin_configuration\n",
    "\n",
    "##################\n",
    "# Tests\n",
    "##################\n",
    "def wrap(pre, post):\n",
    "    def decorate(func):\n",
    "        def call(*args, **kwargs):\n",
    "            pre(func, *args, **kwargs)\n",
    "            result = func(*args, **kwargs)\n",
    "            post(func, result, results, *args, **kwargs)\n",
    "            return result\n",
    "        return call\n",
    "    return decorate\n",
    "\n",
    "def remove_section_digits(name):\n",
    "    filtered = []\n",
    "    name_words = name.split('_')\n",
    "    # remove trailing digits, taking care not to remove 90 in ...than_90_days_4_1_7\n",
    "    for i, word in enumerate(name_words):\n",
    "        if not word.isdigit() or ( (i < len(name_words)-2) and not name_words[i+1].isdigit()):\n",
    "            filtered.append(word)\n",
    "    return '_'.join(filtered)\n",
    "\n",
    "def trace_in(func, *args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def trace_out(func, result, *args, **kwargs):\n",
    "    name = remove_section_digits(func.__name__)\n",
    "    finding_results = results.get(name, {})\n",
    "    if finding_results:\n",
    "        items_flagged_list = finding_results[\"items\"]\n",
    "        items_checked = finding_results[\"stats\"][\"items_checked\"]\n",
    "    else:\n",
    "        items_flagged_list = []\n",
    "        items_checked = 0\n",
    "    items_checked += 1\n",
    "    if not result:\n",
    "        items_flagged_list.append((kwargs['resource_group'], kwargs['server_name']))\n",
    "           \n",
    "    results[name] = {\"items\": items_flagged_list, \"stats\": {\"items_checked\": items_checked}}\n",
    "        \n",
    "results = {}\n",
    "\n",
    "def test_controls() :\n",
    "    global results\n",
    "    sql_servers = load_sql_servers(sql_servers_path)\n",
    "    sql_server_policies = load_sql_server_policies(sql_server_policies_path)\n",
    "    \n",
    "    for (resource_group, server_name), sql_server_policy in sql_server_policies.items():\n",
    "        sql_server_audit_policy = sql_server_policies[(resource_group, server_name)]['audit_policy']\n",
    "        sql_server_threat_detection_policy = sql_server_policies[(resource_group, server_name)]['threat_detection_policy']\n",
    "        sql_server_active_directory_admin_configurations = sql_server_policies[(resource_group, server_name)]['active_directory_admin_configurations']\n",
    "\n",
    "        auditing_is_set_to_on_4_1_1(sql_server_audit_policy, resource_group=resource_group, server_name=server_name)\n",
    "        threat_detection_is_set_to_on_4_1_2(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        threat_detection_types_is_set_to_all_4_1_3(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        send_alerts_to_is_set_4_1_4(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        email_service_and_co_administrators_is_enabled_4_1_5(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        auditing_retention_is_greater_than_90_days_4_1_6(sql_server_audit_policy, resource_group=resource_group, server_name=server_name)\n",
    "        threat_detection_retention_is_greater_than_90_days_4_1_7(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        azure_active_directory_admin_is_configured_4_1_8(sql_server_active_directory_admin_configurations, resource_group=resource_group, server_name=server_name)\n",
    "\n",
    "    stats_results = {}\n",
    "    for finding in results:\n",
    "        items_flagged_list = results[finding][\"items\"]\n",
    "        items_checked = results[finding][\"stats\"][\"items_checked\"]\n",
    "        items_flagged = len(items_flagged_list)\n",
    "        stats = {'items_flagged': len(items_flagged_list),\n",
    "                 'items_checked': items_checked}\n",
    "        metadata = {\"finding_name\": finding,\n",
    "                    \"negative_name\": \"\",\n",
    "                    \"columns\": [\"Region\", \"Server\"]}            \n",
    "        stats_results[finding] = {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "        \n",
    "    with open(os.path.join(scan_data_dir, 'filtered', 'sql_servers_filtered.json'), 'w') as f:\n",
    "        yaml.dump(stats_results, f)\n",
    "    # clear results for next run\n",
    "    results = {}\n",
    "    return stats_results\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def auditing_is_set_to_on_4_1_1(sql_server_audit_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_audit_policies[\"properties\"][\"state\"] == \"Disabled\" :\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def threat_detection_is_set_to_on_4_1_2(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\" :\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "@wrap(trace_in, trace_out)\n",
    "def threat_detection_types_is_set_to_all_4_1_3(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\" or sql_server_threat_detection_policies[\"properties\"][\"disabledAlerts\"] != \"\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def send_alerts_to_is_set_4_1_4(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\" or sql_server_threat_detection_policies[\"properties\"][\"emailAddresses\"] != \"\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def email_service_and_co_administrators_is_enabled_4_1_5(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\" or sql_server_threat_detection_policies[\"properties\"][\"emailAccountAdmins\"] != \"\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def auditing_retention_is_greater_than_90_days_4_1_6(sql_server_audit_policies, resource_group=None, server_name=None):\n",
    "    if (sql_server_audit_policies[\"properties\"][\"state\"] == \"Disabled\"):\n",
    "        return False\n",
    "    retention_days = int(sql_server_audit_policies[\"properties\"][\"retentionDays\"])\n",
    "    if (retention_days) ==0 or (retention_days > 90):\n",
    "        return True\n",
    "    else:    \n",
    "        return False\n",
    "    \n",
    "@wrap(trace_in, trace_out)\n",
    "def threat_detection_retention_is_greater_than_90_days_4_1_7(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if (sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\"):\n",
    "        return False\n",
    "    retention_days = int(sql_server_threat_detection_policies[\"properties\"][\"retentionDays\"])\n",
    "    if (retention_days) ==0 or (retention_days > 90):\n",
    "        return True\n",
    "    else:    \n",
    "        return False\n",
    "    \n",
    "@wrap(trace_in, trace_out)\n",
    "def azure_active_directory_admin_is_configured_4_1_8(sql_server_active_directory_admin_configurations, resource_group=None, server_name=None):\n",
    "    if not sql_server_active_directory_admin_configurations:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile /praetorian-tools/azure_cis_scanner/scanner/sql_databases.py\n",
    "\n",
    "sql_databases_path = os.path.join(raw_data_dir, 'sql_databases.json')\n",
    "sql_database_policies_path = os.path.join(raw_data_dir, 'sql_database_policies.json')\n",
    "sql_databases_filtered_path = os.path.join(scan_data_dir, 'filtered', 'sql_databases_filtered.json')\n",
    "\n",
    "def get_sql_servers(sql_servers_path):\n",
    "    sql_servers = !az sql server list\n",
    "    sql_servers = yaml.load(sql_servers.nlstr)\n",
    "    with open(sql_servers_path, 'w') as f:\n",
    "        yaml.dump(sql_servers, f)\n",
    "    return sql_servers\n",
    "\n",
    "def load_sql_servers(sql_servers_path):\n",
    "    with open(sql_servers_path, 'r') as f:\n",
    "        sql_servers = yaml.load(f)\n",
    "    return sql_servers\n",
    "\n",
    "def get_dbs(sql_servers, sql_databases_path):\n",
    "    server_dbs = {}\n",
    "    for server in sql_servers:\n",
    "        server_name = server['name']\n",
    "        resource_group = server['resourceGroup']\n",
    "        dbs = !az sql db list --resource-group $resource_group --server $server_name\n",
    "        dbs = yaml.load(dbs.nlstr)\n",
    "        server_dbs[(resource_group, server_name)] = dbs\n",
    "        \n",
    "    with open(sql_databases_path, 'w') as f:\n",
    "        yaml.dump(server_dbs, f)\n",
    "    return server_dbs\n",
    "\n",
    "def load_dbs(sql_databases_path):\n",
    "    with open(sql_databases_path, 'r') as f:\n",
    "        server_dbs = yaml.load(f)\n",
    "    return server_dbs\n",
    "\n",
    "def get_sql_database_policies(sql_dbs, sql_database_policies_path):\n",
    "    \"\"\"\n",
    "    For each db in sql_dbs fetch the policies and write to disk\n",
    "    \"\"\"\n",
    "    sql_database_policies = {}\n",
    "    for (resource_group, server_name), dbs in sql_dbs.items():\n",
    "        for db in dbs:\n",
    "            print(db)\n",
    "            db_name = db['name']\n",
    "            if db_name == 'master':\n",
    "                continue\n",
    "            threat_policy = !az sql db threat-policy show --resource-group {resource_group} --server {server_name} --name {db_name}\n",
    "            threat_policy = yaml.load(threat_policy.nlstr)\n",
    "\n",
    "            audit_policy = !az sql db audit-policy show --resource-group {resource_group} --server {server_name} --name {db_name}\n",
    "            audit_policy = yaml.load(audit_policy.nlstr)\n",
    "\n",
    "            tde_policy = !az sql db tde show --resource-group {resource_group} --server {server_name} --database {db_name}\n",
    "            tde_policy = yaml.load(tde_policy.nlstr)\n",
    "\n",
    "            sql_policy = {}\n",
    "            sql_policy['threat'] = threat_policy\n",
    "            sql_policy['audit'] = audit_policy\n",
    "            sql_policy['tde'] = tde_policy\n",
    "            sql_database_policies[(resource_group, server_name, db_name)] = sql_policy\n",
    "        \n",
    "    with open(sql_database_policies_path, 'w') as f:\n",
    "        yaml.dump(sql_database_policies, f)\n",
    "    return sql_database_policies\n",
    "\n",
    "def load_sql_database_policies(sql_policies_path):\n",
    "    \"\"\"\n",
    "    Load sql database policies\n",
    "    \"\"\"\n",
    "    with open(sql_database_policies_path, 'r') as f:\n",
    "        sql_database_policies = yaml.load(f)\n",
    "    return sql_database_policies\n",
    "\n",
    "################\n",
    "# Tests\n",
    "################\n",
    "\n",
    "def remove_section_digits(name):\n",
    "    filtered = []\n",
    "    name_words = name.split('_')\n",
    "    # remove trailing digits, taking care not to remove 90 in ...than_90_days_4_1_7\n",
    "    for i, word in enumerate(name_words):\n",
    "        if not word.isdigit() or ( (i < len(name_words)-2) and not name_words[i+1].isdigit()):\n",
    "            filtered.append(word)\n",
    "    return '_'.join(filtered)\n",
    "    \n",
    "results = {}\n",
    "def test_controls():\n",
    "    global results\n",
    "    def wrap(pre, post):\n",
    "        global results\n",
    "        def decorate(func):\n",
    "            global results\n",
    "            def call(*args, **kwargs):\n",
    "                global results\n",
    "                pre(func, *args, **kwargs)\n",
    "                result = func(*args, **kwargs)\n",
    "                post(func, result, results, *args, **kwargs)\n",
    "                return result\n",
    "            return call\n",
    "        return decorate\n",
    "    \n",
    "    def remove_section_digits(name):\n",
    "        return '_'.join([item for item in name.split('_') if not item.isdigit()])\n",
    "\n",
    "    def trace_in(func, *args, **kwargs):\n",
    "        global results\n",
    "        pass\n",
    "    \n",
    "    def trace_out(func, result, *args, **kwargs):\n",
    "        global results\n",
    "        name = remove_section_digits(func.__name__)\n",
    "        finding_results = results.get(name, {})\n",
    "        if finding_results:\n",
    "            items_flagged_list = finding_results[\"items\"]\n",
    "            items_checked = finding_results[\"stats\"][\"items_checked\"]\n",
    "        else:\n",
    "            items_flagged_list = []\n",
    "            items_checked = 0\n",
    "        items_checked += 1\n",
    "        if not result:\n",
    "            items_flagged_list.append((resource_group, server_name, db))\n",
    "           \n",
    "        results[name] = {\"items\": items_flagged_list, \"stats\": {\"items_checked\": items_checked}}\n",
    "\n",
    "\n",
    "    @wrap(trace_in, trace_out)\n",
    "    def auditing_is_set_to_on_4_2_1(audit_policy):\n",
    "        if audit_policy['state'] != 'Enabled':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "  \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def threat_detection_is_set_to_on_4_2_2(threat_policy):\n",
    "        if threat_policy['state'] != 'Enabled':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def threat_detection_types_is_set_to_all_4_2_3(threat_policy):\n",
    "        if threat_policy['disabledAlerts'] in ['All', '']:\n",
    "            return True\n",
    "        else:\n",
    "            print('threat_detection_types_is_set_to_all_4_2_3 disabledAlerts', threat_policy['disabledAlerts'])\n",
    "            return False\n",
    "        \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def send_alerts_to_is_set_4_2_4(threat_policy):\n",
    "        if threat_policy['emailAddresses']:\n",
    "            return set(threat_policy['emailAddresses'])\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def email_service_and_co_administrators_is_enabled_4_2_5(threat_policy):\n",
    "        if threat_policy['emailAccountAdmins'] != \"Enabled\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def data_encryption_is_set_to_on_4_2_6(tde_policy):\n",
    "        if tde_policy['status'] != \"Enabled\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    @wrap(trace_in, trace_out)\n",
    "    def auditing_retention_is_greater_than_90_days_4_2_7(audit_policy):\n",
    "        if (audit_policy['retentionDays'] > 0) and (audit_policy['retention_days'] <= 90):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    @wrap(trace_in, trace_out)\n",
    "    def threat_retention_is_greater_than_90_days_4_2_8(threat_policy):\n",
    "        if (threat_policy['retentionDays'] > 0) and (threat_policy['retention_days'] == 0) <= 90:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    sql_database_policies = load_sql_database_policies(sql_database_policies_path)\n",
    "    for (resource_group, server_name, db), sql_database_policy in sql_database_policies.items():\n",
    "        \n",
    "        audit_policy = sql_database_policy['audit']\n",
    "        threat_policy = sql_database_policy['threat']\n",
    "        tde_policy = sql_database_policy['tde']            \n",
    "        \n",
    "        auditing_is_set_to_on_4_2_1(audit_policy)\n",
    "        threat_detection_is_set_to_on_4_2_2(threat_policy)\n",
    "        threat_detection_types_is_set_to_all_4_2_3(threat_policy)\n",
    "        send_alerts_to_is_set_4_2_4(threat_policy)\n",
    "        email_service_and_co_administrators_is_enabled_4_2_5(threat_policy)\n",
    "        data_encryption_is_set_to_on_4_2_6(tde_policy)\n",
    "        auditing_retention_is_greater_than_90_days_4_2_7(audit_policy)\n",
    "        threat_retention_is_greater_than_90_days_4_2_8(threat_policy)\n",
    "\n",
    "    stats_results = {}\n",
    "    for finding in results:\n",
    "        items_flagged_list = results[finding][\"items\"]\n",
    "        items_checked = results[finding][\"stats\"][\"items_checked\"]\n",
    "        items_flagged = len(items_flagged_list)\n",
    "        stats = {'items_flagged': len(items_flagged_list),\n",
    "                 'items_checked': items_checked}\n",
    "        metadata = {\"finding_name\": finding,\n",
    "                    \"negative_name\": \"\",\n",
    "                    \"columns\": [\"Region\", \"Server\", \"Database\"]}            \n",
    "        stats_results[finding] = {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "    \n",
    "    with open(sql_databases_filtered_path, 'w') as f:\n",
    "        yaml.dump(stats_results, f)\n",
    "    # clear results for next run\n",
    "    results = {}\n",
    "    return stats_results\n",
    "\n",
    "def get_data():\n",
    "    sql_servers = get_sql_servers(sql_servers_path)\n",
    "    sql_dbs = get_dbs(sql_servers, sql_databases_path)\n",
    "    \n",
    "    sql_database_policies = get_sql_database_policies(sql_dbs, sql_database_policies_path)\n",
    "\n",
    "    return {\"sql_servers\": sql_servers, \"sql_databases\": sql_dbs, \"sql_database_policies\": sql_database_policies}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sql_databases_path)\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Logging and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate files in raw_data_dir\n",
    "\n",
    "monitor_diagnostic_settings_path = os.path.join(raw_data_dir, 'monitor_diagnostic_settings.json')\n",
    "activity_logs_path = os.path.join(raw_data_dir, 'activity_logs.json')\n",
    "\n",
    "resource_ids_for_diagnostic_settings_path = os.path.join(raw_data_dir, 'resource_ids_for_diagnostic_settings.json')\n",
    "resource_diagnostic_settings_path = os.path.join(raw_data_dir, 'resource_diagnostic_settings.json')\n",
    "\n",
    "logging_and_monitoring_filtered_path = os.path.join(filtered_data_dir, 'logging_and_monitoring_filtered.json')\n",
    "\n",
    "def get_resource_ids_for_diagnostic_settings():\n",
    "    resource_ids = []\n",
    "    # Other resource_ids could be gathered.  So far, only keyvault\n",
    "    keyvaults = !az keyvault list    \n",
    "    keyvaults = yaml.load(keyvaults.nlstr)\n",
    "    for keyvault in keyvaults:\n",
    "        resource_ids.append(keyvault['id'])\n",
    "    with open(resource_ids_for_diagnostic_settings_path, 'w') as f:\n",
    "        json.dump(resource_ids, f, indent=4, sort_keys=True)\n",
    "    return resource_ids\n",
    "\n",
    "def load_resource_ids_for_diagnostic_settings(resource_ids_for_diagnostic_settings_path):\n",
    "    with open(resource_ids_for_diagnostic_settings_path, 'r') as f:\n",
    "        resource_ids_for_diagnostic_settings = yaml.load(f)\n",
    "    return resource_ids_for_diagnostic_settings\n",
    "\n",
    "def get_resource_diagnostic_settings(resource_ids_for_diagnostic_settings):\n",
    "    keyvault_settings_list = []\n",
    "    for resource_id in resource_ids_for_diagnostic_settings:\n",
    "        keyvault_settings = !az monitor diagnostic-settings list --resource {resource_id}\n",
    "        keyvault_settings = yaml.load(keyvault_settings.nlstr)\n",
    "        *prefix, resource_group, _, _, _, keyvault_name = resource_id.split('/')\n",
    "        for setting in keyvault_settings['value']:\n",
    "            print('settings: ', keyvault_settings)\n",
    "            setting['keyvault_name'] = keyvault_name\n",
    "        keyvault_settings_list.append(keyvault_settings)\n",
    "        \n",
    "    with open(resource_diagnostic_settings_path, 'w') as f:\n",
    "        yaml.dump(keyvault_settings_list, f)\n",
    "    return resource_ids_for_diagnostic_settings \n",
    "\n",
    "def load_resource_diagnostic_settings(resource_diagnostic_settings_path):\n",
    "    with open(resource_diagnostic_settings_path, 'r') as f:\n",
    "        resource_diagnostic_settings = yaml.load(f)\n",
    "    return resource_diagnostic_settings        \n",
    "        \n",
    "def get_monitor_diagnostic_settings(monitor_diagnostic_settings_path, resource_ids):\n",
    "    \"\"\"\n",
    "    @monitor_diagnostic_settings_path: string - path to output json file\n",
    "    @returns: list of activity_log_alerts dicts\n",
    "    \"\"\"\n",
    "    monitor_diagnostic_settings_results = {}\n",
    "    for resource_id in resource_ids:\n",
    "        monitor_diagnostic_settings = !az monitor diagnostic-settings list --resource {resource_id}\n",
    "        monitor_diagnostic_settings = yaml.load(monitor_diagnostic_settings.nlstr)\n",
    "        monitor_diagnostic_settings_results[resource_id] = monitor_diagnostic_settings\n",
    "    with open(monitor_diagnostic_settings_path, 'w') as f:\n",
    "        json.dump(monitor_diagnostic_settings_results, f, indent=4, sort_keys=True)\n",
    "    return monitor_diagnostic_settings_results\n",
    "\n",
    "def load_monitor_diagnostic_settings(monitor_diagnostic_settings_path):\n",
    "    with open(monitor_diagnostic_settings_path, 'r') as f:\n",
    "        monitor_diagnostic_settings = yaml.load(f)\n",
    "    return monitor_diagnostic_settings\n",
    "\n",
    "monitor_log_profiles_path = os.path.join(raw_data_dir, 'monitor_log_profiles.json')\n",
    "\n",
    "def get_monitor_log_profiles(monitor_log_profiles_path):\n",
    "    monitor_log_profiles = !az monitor log-profiles list\n",
    "    monitor_log_profiles = yaml.load(monitor_log_profiles.nlstr)\n",
    "    with open(monitor_log_profiles_path, 'w') as f:\n",
    "        json.dump(monitor_log_profiles, f, indent=4, sort_keys=True)\n",
    "    return monitor_log_profiles\n",
    "\n",
    "def load_monitor_log_profiles(monitor_log_profiles_path):\n",
    "    with open(monitor_log_profiles_path, 'r') as f:\n",
    "        monitor_log_profiles = yaml.load(f)\n",
    "    return monitor_log_profiles\n",
    "\n",
    "\n",
    "activity_logs_starttime_timedelta = datetime.timedelta(days=90)\n",
    "def get_start_time(timedelta=datetime.timedelta(days=90)):\n",
    "    \"\"\"\n",
    "    Given datetime.timedelta(days=days, hours=hours), return string in iso tz format \n",
    "    \"\"\"\n",
    "    return datetime.datetime.strftime(datetime.datetime.now() - timedelta, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def get_activity_logs(activity_logs_path, resource_groups):\n",
    "    activity_logs = {}\n",
    "    start_time = get_start_time(activity_logs_starttime_timedelta)\n",
    "    for resource_group in resource_groups:\n",
    "        resource_group = resource_group['name']\n",
    "        activity_log = !az monitor activity-log list --resource-group {resource_group} --start-time {start_time}\n",
    "        activity_log = yaml.load(activity_log.nlstr)\n",
    "        activity_logs[resource_group] = activity_log\n",
    "    with open(activity_logs_path, 'w') as f:\n",
    "        json.dump(activity_logs, f, indent=4, sort_keys=True)\n",
    "    return activity_logs    \n",
    "\n",
    "def load_activity_logs(activity_logs_path):\n",
    "    with open(activity_logs_path, 'r') as f:\n",
    "        activity_log = yaml.load(f)\n",
    "    return activity_log\n",
    "\n",
    "activity_log_alerts_path = os.path.join(raw_data_dir, 'activity_log_alerts.json')\n",
    "\n",
    "def get_activity_log_alerts(activity_log_alerts_path):\n",
    "    activity_log_alerts = !az monitor activity-log alert list\n",
    "    activity_log_alerts = yaml.load(activity_log_alerts.nlstr)\n",
    "    with open(activity_log_alerts_path, 'w') as f:\n",
    "        json.dump(activity_log_alerts, f, indent=4, sort_keys=True)\n",
    "    return activity_log_alerts   \n",
    "\n",
    "def load_activity_log_alerts(activity_log_alerts_path):\n",
    "    with open(activity_log_alerts_path, 'r') as f:\n",
    "        activity_log_alerts = yaml.load(f)\n",
    "    return activity_log_alerts\n",
    "\n",
    "def get_data():\n",
    "    resource_ids_for_diagnostic_settings = get_resource_ids_for_diagnostic_settings()\n",
    "    resource_groups = get_resource_groups(resource_groups_path)\n",
    "    get_monitor_log_profiles(monitor_log_profiles_path)\n",
    "    get_monitor_diagnostic_settings(monitor_diagnostic_settings_path, resource_ids_for_diagnostic_settings)\n",
    "    get_activity_log_alerts(activity_log_alerts_path)\n",
    "    get_activity_logs(activity_logs_path, resource_groups)\n",
    "    get_resource_diagnostic_settings(resource_ids_for_diagnostic_settings)\n",
    "\n",
    "    \n",
    "    \n",
    "##################\n",
    "# Tests\n",
    "##################\n",
    "\n",
    "def test_controls():\n",
    "    \"\"\"\n",
    "    Use the data in raw_data_dir or in memory to run tests.\n",
    "    Filtered output of raw_data_dir for failing systems is placed in filtered_data_dir\n",
    "    \"\"\"\n",
    "    resource_ids_for_diagnostic_settings = load_resource_ids_for_diagnostic_settings(resource_ids_for_diagnostic_settings_path)\n",
    "    resource_diagnostic_settings = load_resource_diagnostic_settings(resource_diagnostic_settings_path)\n",
    "    resource_groups = load_resource_groups(resource_groups_path)\n",
    "    monitor_log_profiles = load_monitor_log_profiles(monitor_log_profiles_path)\n",
    "    monitor_diagnostic_settings = load_monitor_diagnostic_settings(monitor_diagnostic_settings_path)\n",
    "    activity_log_alerts = load_activity_log_alerts(activity_log_alerts_path)\n",
    "    activity_logs = load_activity_logs(activity_logs_path)\n",
    "    \n",
    "    \n",
    "    results = {}\n",
    "    results['a_log_profile_exists'] = a_log_profile_exists_5_1(monitor_log_profiles)\n",
    "    results['activity_log_retention_is_set_365_days_or_greater'] = activity_log_retention_is_set_365_days_or_greater_5_2(monitor_log_profiles)\n",
    "    results['activity_log_alert_is_configured'] = activity_log_alert_is_configured(activity_log_alerts, log_alert_policies)\n",
    "    results['logging_for_azure_keyvault_is_enabled'] = logging_for_azure_keyvault_is_enabled_5_13(resource_diagnostic_settings)\n",
    "\n",
    "    with open(logging_and_monitoring_filtered_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4, sort_keys=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def a_log_profile_exists_5_1(monitor_log_profiles):\n",
    "    items_flagged_list = []\n",
    "    if monitor_log_profiles:\n",
    "        pass\n",
    "    else:\n",
    "        items_flagged_list.append((\"No log profile\"))\n",
    "        \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': min(1, len(items_flagged_list))}\n",
    "    metadata = {\"finding_name\": \"a_log_profile_exists\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Monitor Log Profile\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "    \n",
    "\n",
    "# Todo, untested as we have [] for log-profiles\n",
    "#@gen_results(results)\n",
    "def activity_log_retention_is_set_365_days_or_greater_5_2(monitor_log_profiles):\n",
    "    items_flagged_list = []\n",
    "    if monitor_log_profiles:\n",
    "        for profile in monitor_log_profiles:\n",
    "            if monitor_log_profiles['retentionPolicy'] <= MIN_ACTIVITY_LOG_RETENDION_DAYS:\n",
    "                items_flagged_list.append((profile))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(monitor_log_profiles) if monitor_log_profiles else 1}\n",
    "    metadata = {\"finding_name\": \"activity_log_retention_is_set_365_days_or_greater\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Monitor Log Profile\", \"Retention Days\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "# 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 5.10, 5.11, 5.12    \n",
    "log_alert_policies_str = '''\n",
    "- alert_name: 'create_policy_assignment'\n",
    "  operation_name: 'Microsoft.Authorization/policyAssignments/write'\n",
    "  present: False\n",
    "- alert_name: 'create_or_update_network_security_group'\n",
    "  operation_name: 'Microsoft.Network/networkSecurityGroups/write'\n",
    "  present: False\n",
    "- alert_name: 'delete_network_security_group'\n",
    "  operation_name: 'Microsoft.Network/networkSecurityGroups/delete'\n",
    "  present: False\n",
    "- alert_name: 'create_or_update_network_security_group_rule'\n",
    "  operation_name: 'Microsoft.Network/networkSecurityGroups/securityRules/write'\n",
    "  present: False\n",
    "- alert_name: 'delete_network_security_group_rule'\n",
    "  operation_name: 'Microsoft.Network/networkSecurityGroups/securityRules/delete'\n",
    "  present: False\n",
    "- alert_name: 'create_or_update_security_solution'\n",
    "  operation_name: 'Microsoft.Security/securitySolutions/write'\n",
    "  present: False\n",
    "- alert_name: 'delete_security_solution'\n",
    "  operation_name: 'Microsoft.Security/securitySolutions/delete'\n",
    "  present: False\n",
    "- alert_name: 'update_or_create_SQL_server_firewall_rule'\n",
    "  operation_name: 'Microsoft.Sql/servers/firewallRules/write'\n",
    "  present: False\n",
    "- alert_name: 'delete_SQL_server_firewall_rule'\n",
    "  operation_name: 'Microsoft.Sql/servers/firewallRules/delete'\n",
    "  present: False\n",
    "- alert_name: 'update_security_policy'\n",
    "  operation_name: 'Microsoft.Security/policies/write'\n",
    "  present: False\n",
    "'''\n",
    "log_alert_policies = yaml.load(log_alert_policies_str)\n",
    "\n",
    "def activity_log_alert_is_configured(activity_log_alerts, log_alert_policies):\n",
    "    \"\"\"\n",
    "    #TODO WIP\n",
    "    For each resource_group determine if activity-log alerts are configured correctly\n",
    "    @returns: list of [resource_group, True of False for 5.3 to 5.12 in succession]\n",
    "    \"\"\"\n",
    "    items_flagged_list = []\n",
    "\n",
    "  \n",
    "    for log_alert in activity_log_alerts:\n",
    "        condition = log_alert.get('condition', [])\n",
    "        if not condition:\n",
    "            continue\n",
    "        conditions = condition.get('allOf', [])\n",
    "        if not conditions:\n",
    "            continue\n",
    "        for condition in conditions:\n",
    "            for log_alert_policy in log_alert_policies:\n",
    "                if condition.get('equals') and (condition.get('equals') == log_alert_policy[\"operation_name\"]):\n",
    "                    log_alert_policy[\"present\"] = True\n",
    "\n",
    "    for log_alert_policy in log_alert_policies:\n",
    "        if log_alert_policy['present'] == False:\n",
    "            items_flagged_list.append((log_alert_policy['alert_name'], log_alert_policy['operation_name']))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(log_alert_policies)}\n",
    "    metadata = {\"finding_name\": \"activity_log_alert_is_configured\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Missing Policy\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "                                                                    \n",
    "#@gen_results(results)\n",
    "MIN_ACTIVITY_LOG_RETENDION_DAYS = 365\n",
    "MIN_KEY_VAULT_RETENTION_DAYS = 180\n",
    "def logging_for_azure_keyvault_is_enabled_5_13(resource_diagnostic_settings):        \n",
    "    items_flagged_list = []\n",
    "    for setting in resource_diagnostic_settings:\n",
    "        keyvault_settings_values = setting['value']\n",
    "        if keyvault_settings_values:\n",
    "            for value in keyvault_settings_values:\n",
    "                # Do we need to loop over ['logs'] list as well?  My lists are length 1, only checking [0]\n",
    "                keyvault_name = value['keyvault_name']\n",
    "                enabled = value['logs'][0]['enabled']\n",
    "                retention_enabled = value['logs'][0]['retentionPolicy']['enabled']\n",
    "                retention_days = value['logs'][0]['retentionPolicy']['days']\n",
    "                if not (enabled and retention_enabled and (retention_days >= MIN_KEY_VAULT_RETENTION_DAYS)):\n",
    "                    items_flagged_list.append((keyvault_name, enabled, retention_enabled, retention_days))\n",
    "        else:\n",
    "            items_flagged_list.append((resource_group, keyvault_name, \"False\", \"False\", \"None\"))\n",
    "            \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(resource_diagnostic_settings)}\n",
    "    metadata = {\"finding_name\": \"logging_for_azure_keyvault_is_enabled\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Keyvault\", \"Enabled\", \"Retention Enabled\", \"Retention Days\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Networking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Get Raw Data\n",
    "##########################\n",
    "\n",
    "network_flows_path = os.path.join(raw_data_dir, \"network_flows.json\")\n",
    "resource_groups_path = os.path.join(raw_data_dir, \"resource_groups.json\")\n",
    "networking_filtered_path = os.path.join(scan_data_dir, 'filtered', 'networking_filtered.json')\n",
    "\n",
    "network_security_groups_path = os.path.join(raw_data_dir, \"network_security_groups.json\")\n",
    "\n",
    "def get_data():\n",
    "    get_resource_groups(resource_groups_path)\n",
    "    network_security_groups = get_network_security_groups(network_security_groups_path)\n",
    "    get_network_watcher(network_watcher_path)\n",
    "    get_network_flows(network_flows_path, network_security_groups)\n",
    "    \n",
    "def get_network_security_groups(network_security_groups_path):\n",
    "    \"\"\"\n",
    "    @network_path: string - path to output json file\n",
    "    \"\"\"\n",
    "    network_security_groups = !az network nsg list\n",
    "    network_security_groups = yaml.load(network_security_groups.nlstr)\n",
    "    with open(network_security_groups_path, 'w') as f:\n",
    "        json.dump(network_security_groups, f, indent=4, sort_keys=True)\n",
    "    return network_security_groups\n",
    "\n",
    "def load_network_security_groups(network_security_groups_path):\n",
    "    with open(network_security_groups_path, 'r') as f:\n",
    "        network_security_groups = yaml.load(f)\n",
    "    return network_security_groups\n",
    "\n",
    "network_watcher_path = os.path.join(raw_data_dir, \"network_watcher.json\")\n",
    "approved_regions = []\n",
    "def get_network_watcher(network_watcher_path):\n",
    "    \"\"\"\n",
    "    @network_watcher_path: string - path to output json file\n",
    "    \"\"\"\n",
    "    network_watcher = !az network watcher list\n",
    "    network_watcher = yaml.load(network_watcher.nlstr)\n",
    "    with open(network_watcher_path, 'w') as f:\n",
    "        json.dump(network_watcher, f, indent=4, sort_keys=True)\n",
    "    return network_watcher\n",
    "\n",
    "def load_network_watcher(network_watcher_path):\n",
    "    with open(network_watcher_path, 'r') as f:\n",
    "        network_watcher = yaml.load(f)\n",
    "    return network_watcher\n",
    "\n",
    "def get_network_flows(network_flows_path, network_security_groups):\n",
    "    \"\"\"\n",
    "    @network_flows_path: string - path to output json file\n",
    "    @network_security_groups: list of nsgs\n",
    "    @returns: list of network flow dicts\n",
    "    \"\"\"\n",
    "    network_flows = []\n",
    "    for nsg in network_security_groups:\n",
    "        resource_group = nsg['resourceGroup']\n",
    "        nsg_id = nsg['id']\n",
    "        network_flow = !az network watcher flow-log show --resource-group {resource_group} --nsg {nsg_id}\n",
    "        network_flow = yaml.load(network_flow.nlstr)\n",
    "        nsg_name = nsg[\"name\"]\n",
    "        network_flows.append({\"resource_group\": resource_group, \"nsg_name\": nsg_name, \"network_flow\": network_flow})\n",
    "        \n",
    "    with open(network_flows_path, 'w') as f:\n",
    "        json.dump(network_flows, f, indent=4, sort_keys=True)\n",
    "    return network_flows\n",
    "\n",
    "def load_network_flows(network_flows_path):\n",
    "    with open(network_flows_path, 'r') as f:\n",
    "        network_flows = yaml.load(f)\n",
    "    return network_flows\n",
    "\n",
    "##########################\n",
    "# Tests\n",
    "##########################\n",
    "\n",
    "def test_controls():\n",
    "    \"\"\"\n",
    "    Generate filtered (failing) output in json\n",
    "    \"\"\"\n",
    "    network_watcher = load_network_watcher(network_watcher_path)\n",
    "    network_security_groups = load_network_security_groups(network_security_groups_path)\n",
    "    resource_groups = load_resource_groups(resource_groups_path)\n",
    "    network_flows = load_network_flows(network_flows_path)\n",
    "    networking_results = {}\n",
    "\n",
    "    networking_results['access_is_restricted_from_the_internet'] = access_is_restricted_from_the_internet_6_1(network_security_groups)\n",
    "    networking_results['network_security_group_flow_log_retention_period_is_greater_than_90_days'] = network_security_group_flow_log_retention_period_is_greater_than_90_days_6_4(network_flows)\n",
    "    networking_results['network_watcher_is_enabled'] = network_watcher_is_enabled_6_5(network_watcher)\n",
    "                \n",
    "    with open(networking_filtered_path, 'w') as f:\n",
    "        json.dump(networking_results, f, indent=4, sort_keys=True)\n",
    "    return networking_results\n",
    "\n",
    "# 6.1, 6.2, \n",
    "def access_is_restricted_from_the_internet_6_1(network_security_groups):\n",
    "    items_flagged_list = []\n",
    "    for nsg in network_security_groups:\n",
    "        # should actually be any port range that includes 3389\n",
    "        security_rules = nsg['securityRules']\n",
    "        for security_rule in security_rules:\n",
    "            if security_rule['destinationPortRange'] == '3389' and security_rule['direction'] == 'Inbound' and security_rule['protocol'] == 'TCP':\n",
    "                if security_rule['sourceAddressPrefix'] in ['*', '/0', 'internet', 'any']:\n",
    "                    items_flagged_list.append((nsg['resourceGroup'],nsg['name'], '3389', security_rule))\n",
    "            if security_rule['destinationPortRange'] == '22' and security_rule['direction'] == 'Inbound':\n",
    "                if security_rule['sourceAddressPrefix'] in ['*', '/0', 'internet', 'any']:\n",
    "                    items_flagged_list.append((nsg['resourceGroup'],nsg['name'], '22', security_rule))\n",
    "                    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(network_security_groups)}\n",
    "    metadata = {\"finding_name\": \"access_is_restricted_from_the_internet\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"NSG\", \"Port\", \"Rule\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "\n",
    "def sql_server_access_is_restricted_from_the_internet_6_3():\n",
    "    \"\"\"\n",
    "    Powershell\n",
    "    \"\"\"\n",
    "    pass                \n",
    "\n",
    "def network_security_group_flow_log_retention_period_is_greater_than_90_days_6_4(network_flows):\n",
    "    items_flagged_list = []\n",
    "    for network_flow in network_flows:\n",
    "        flow = network_flow['network_flow']\n",
    "        if flow['enabled'] == False:\n",
    "            status = \"Not enabled\"\n",
    "            items_flagged_list.append((network_flow['resource_group'], network_flow['nsg_name'], status))\n",
    "        elif flow['retentionPolicy']['days'] == 0:\n",
    "            continue\n",
    "        elif (flow['retentionPolicy']['days'] < 90) or (flow['retentionPolicy']['enabled'] == False):\n",
    "            status(\"Days {}, Enabled {}\".format(flow['retentionPolicy']['days'], flow['retentionPolicy']['enabled']))\n",
    "            items_flagged_list.append((network_flow['resource_group'], network_flow['nsg_name'], status))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(network_flows)}\n",
    "    metadata = {\"finding_name\": \"network_security_group_flow_log_retention_period_is_greater_than_90_days\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Network Flow\", \"Status\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def network_watcher_is_enabled_6_5(network_watcher):\n",
    "    items_flagged_list = []    \n",
    "    for watcher in network_watcher:\n",
    "        if watcher['provisioningState'] != 'Succeeded':\n",
    "            items_flagged_list.append((watcher))\n",
    "            \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(network_watcher)}\n",
    "    metadata = {\"finding_name\": \"network_security_group_flow_log_retention_period_is_greater_than_90_days\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Network Flow\", \"Status\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware of permission errors visible from the cli\n",
    "\n",
    "$ az network watcher flow-log show -g <my-group> --nsg <my-nsg>\n",
    "The client 'kesten.Broughton@texascapitalbank.com' with object id 'abdcxxx-asdlj-fdskl-yyy-asfd17' does not have authorization to perform action 'Microsoft.Network/networkWatchers/queryFlowLogStatus/action' over scope '/subscriptions/exxxxxx-cbbbb-4444-bbbb-aaaaaaa2e3/resourceGroups/NetworkWatcherRG/providers/Microsoft.Network/networkWatchers/NetworkWatcher_southcentralus'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_finding(findings_template_path, r_parsed, 'Virtual Machines', 2, output='', findings_output_path=findings_out_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /praetorian-tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual Machines\n",
    "\n",
    "filtered_virtual_machines_path = os.path.join(filtered_data_dir, 'virtual_machines_filtered.json')\n",
    "virtual_machines_path = os.path.join(raw_data_dir, 'virtual_machines.json')\n",
    "\n",
    "def get_virtual_machines(virtual_machines_path):\n",
    "    \"\"\"\n",
    "    @virtual_machines_path: string - path to output json file\n",
    "    @returns: list of virtual_machines dict\n",
    "    \"\"\"\n",
    "    virtual_machines = !az vm list\n",
    "    virtual_machines = yaml.load(virtual_machines.nlstr)\n",
    "    with open(virtual_machines_path, 'w') as f:\n",
    "        json.dump(virtual_machines, f, indent=4, sort_keys=True)\n",
    "    return virtual_machines\n",
    "\n",
    "def load_virtual_machines(virtual_machines_path):\n",
    "    with open(virtual_machines_path, 'r') as f:\n",
    "        virtual_machines = yaml.load(f)\n",
    "    return virtual_machines\n",
    "\n",
    "def get_data():\n",
    "    get_virtual_machines(virtual_machines_path)\n",
    "\n",
    "def vm_agent_is_installed_7_1(virtual_machines):\n",
    "    items_flagged_list = []\n",
    "    for vm in virtual_machines:\n",
    "        has_agent = False\n",
    "        if vm['resources']:\n",
    "            for resource in vm[\"resources\"]:\n",
    "                if ((vm['resources'][0]['virtualMachineExtensionType'] == 'MicrosoftMonitoringAgent') and (vm['resources'][0]['provisioningState'] == 'Succeeded')):\n",
    "                    has_agent = True\n",
    "        if has_agent:\n",
    "            items_flagged_list.append((vm['resourceGroup'], vm['name']))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"vm_agent_is_installed\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def os_disk_is_encrypted_7_2(virtual_machines):\n",
    "    items_flagged_list = []\n",
    "    items_checked = 0\n",
    "    for vm in virtual_machines:\n",
    "        if vm['storageProfile']['osDisk']['encryptionSettings']:\n",
    "            if not (vm['storageProfile']['osDisk']['encryptionSettings']['enabled'] == True):\n",
    "                items_flagged_list.append((vm['resourceGroup'], vm['name'], vm['storageProfile']['osDisk']['name']))\n",
    "                items_checked += 1\n",
    "        else:\n",
    "            items_flagged_list.append((vm['resourceGroup'], vm['name'], vm['storageProfile']['osDisk']['name']))\n",
    "            items_checked += 1\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"os_disk_is_encrypted\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Name\", \"Disk Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def data_disks_are_encrypted_7_3(virtual_machines):\n",
    "    items_flagged_list = []\n",
    "    items_checked = 0\n",
    "    for vm in virtual_machines:\n",
    "        name = vm['name']\n",
    "        resource_group = vm['resourceGroup']\n",
    "#         encrypted = !az vm encryption show --name {name} --resource-group {resource_group} --query dataDisk\n",
    "#         encrypted = yaml.load(encrypted.nlstr)\n",
    "#         if encrypted != \"Encrypted\":\n",
    "#             items_flagged_list.append((vm['resourceGroup'], vm['name']))\n",
    "        for disk in vm['storageProfile']['dataDisks']:\n",
    "            if disk['encryptionSettings'] == None:\n",
    "                items_flagged_list.append((vm['resourceGroup'], vm['name'], disk['name']))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"data_disks_are_encrypted\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Name\", \"Disk Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "\n",
    "def only_approved_extensions_are_installed_7_4(virtual_machines):\n",
    "    # items in the following list do not imply failure, but require review\n",
    "    items_flagged_list = []\n",
    "    approved_extensions = [\n",
    "        'AzureDiskEncryption',\n",
    "        'IaaSAntimalware',\n",
    "        'IaaSDiagnostics',\n",
    "        'MicrosoftMonitoringAgent',\n",
    "        'SqlIaaSAgent',\n",
    "        'OmsAgentForLinux', \n",
    "        'VMAccessForLinux',\n",
    "    ]\n",
    "    for vm in virtual_machines:\n",
    "        name = vm['name']\n",
    "        resource_group = vm['resourceGroup']\n",
    "        extensions = !az vm extension list --vm-name {name} --resource-group {resource_group}\n",
    "        extensions = yaml.load(extensions.nlstr)\n",
    "        for extension in extensions:\n",
    "            if extension['virtualMachineExtensionType'] not in approved_extensions:\n",
    "                items_flagged_list.append((resource_group, name, extension['virtualMachineExtensionType']))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"only_approved_extensions_are_installed\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"VM Name\", \"Extension Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def latest_patches_for_all_virtual_machines_are_applied_7_5(virtual_machines):\n",
    "    pass\n",
    "\n",
    "def endpoint_protection_for_all_virtual_machines_is_installed_7_6(virtual_machines):\n",
    "    items_flagged_list = []\n",
    "    accepted_protections = set(['EndpointSecurity', 'TrendMicroDSA', 'Antimalware', 'EndpointProtection','SCWPAgent', 'PortalProtectExtension', 'FileSecurity', 'IaaSAntimalware'])\n",
    "    for vm in virtual_machines:\n",
    "        name = vm['name']\n",
    "        resource_group = vm['resourceGroup']\n",
    "#         endpoint_protection = !az vm show --resource-group {resource_group} --name {name} -d\n",
    "#         endpoint_protection = yaml.load(endpoint_protection.nlstr)\n",
    "        extensions = !az vm extension list --vm-name {name} --resource-group {resource_group}\n",
    "        extensions = yaml.load(extensions.nlstr)\n",
    "        has_protection = False\n",
    "        for extension in extensions:\n",
    "            if set([extension['virtualMachineExtensionType']]).intersection(accepted_protections):\n",
    "                has_protection = True\n",
    "        if not has_protection:\n",
    "            items_flagged_list.append((resource_group, name, extension.get('virtualMachineExtensionType', \"No extension\")))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"endpoint_protection_for_all_virtual_machines_is_installed\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Name\", \"Unapproved Extension\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "\n",
    "def test_controls():\n",
    "    results = {}\n",
    "    virtual_machines = load_virtual_machines(virtual_machines_path)\n",
    "    results['vm_agent_is_installed'] = vm_agent_is_installed_7_1(virtual_machines)\n",
    "    results['os_disk_is_encrypted'] = os_disk_is_encrypted_7_2(virtual_machines)\n",
    "    results['data_disks_are_encrypted'] = data_disks_are_encrypted_7_3(virtual_machines)\n",
    "    results['only_approved_extensions_are_installed'] = only_approved_extensions_are_installed_7_4(virtual_machines)\n",
    "    results['endpoint_protection_for_all_virtual_machines_is_installed'] = endpoint_protection_for_all_virtual_machines_is_installed_7_6(virtual_machines)\n",
    "    \n",
    "    with open(filtered_virtual_machines_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4, sort_keys=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Security Considerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvaults_path = os.path.join(raw_data_dir, 'keyvaults.json')\n",
    "keyvault_keys_and_secrets_metadata_path = os.path.join(raw_data_dir, 'keyvault_keys_and_secrets_metadata.json')\n",
    "locked_resources_path = os.path.join(raw_data_dir, 'locked_resources.json')\n",
    "other_security_considerations_filtered_path = os.path.join(filtered_data_dir, 'other_security_considerations_filtered.json')\n",
    "\n",
    "def get_keyvaults(keyvaults_path):\n",
    "    \"\"\"\n",
    "    @keyvaults_path: string - path to output json file\n",
    "    @returns: list of virtual_machines dict\n",
    "    \"\"\"\n",
    "    keyvaults = !az keyvault list\n",
    "    keyvaults = yaml.load(keyvaults.nlstr)\n",
    "    with open(keyvaults_path, 'w') as f:\n",
    "        json.dump(keyvaults, f, indent=4, sort_keys=True)\n",
    "    return keyvaults\n",
    "\n",
    "def load_keyvaults(keyvaults_path):\n",
    "    with open(keyvaults_path, 'r') as f:\n",
    "        keyvaults = yaml.load(f)\n",
    "    return keyvaults\n",
    "\n",
    "def get_locked_resources():\n",
    "    lock_list = !az lock list\n",
    "    lock_list = yaml.load(lock_list.nlstr)\n",
    "\n",
    "    with open(locked_resources_path, 'w') as f:\n",
    "        json.dump(lock_list, f, indent=4, sort_keys=True)\n",
    "    return lock_list\n",
    "\n",
    "def load_locked_resources(locked_resources_path):\n",
    "    with open(locked_resources_path, 'r') as f:\n",
    "        locked_list = yaml.load(f)\n",
    "    return locked_list\n",
    "\n",
    "def get_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path, keyvaults):\n",
    "    metadata = {}\n",
    "    for keyvault in keyvaults:        \n",
    "        vault_name = keyvault['name']\n",
    "        metadata[vault_name] = {}\n",
    "        keys = !az keyvault key list --vault-name {vault_name}\n",
    "        keys = yaml.load(keys.nlstr)\n",
    "        metadata[vault_name]['keys'] = keys\n",
    "        secrets = !az keyvault secret list --vault-name {vault_name}\n",
    "        secrets = yaml.load(secrets.nlstr)\n",
    "        metadata[vault_name]['secrets'] = secrets\n",
    "    \n",
    "    with open(keyvault_keys_and_secrets_metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4, sort_keys=True)\n",
    "    return metadata\n",
    "\n",
    "def load_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path):\n",
    "    with open(keyvault_keys_and_secrets_metadata_path, 'r') as f:\n",
    "        metadata = yaml.load(f)\n",
    "    return metadata\n",
    "\n",
    "def get_data():\n",
    "    keyvaults = get_keyvaults(keyvaults_path)\n",
    "    get_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path, keyvaults)\n",
    "    get_locked_resources()\n",
    "\n",
    "MAX_EXPIRY_ROTATION_DAYS = 730\n",
    "# 8.1 and 8.2\n",
    "def expiry_date_is_set_on_all_keys_and_secrets(keyvault_keys_and_secrets_metadata):\n",
    "    items_flagged_list = []\n",
    "    items_checked = 0\n",
    "    today = datetime.datetime.today()\n",
    "    \n",
    "    def get_key_or_secret_status(info):\n",
    "        enabled = info['attributes']['enabled']\n",
    "        created = datetime.datetime.strptime(info['attributes']['created'].split('T')[0], '%Y-%m-%d')\n",
    "        expires = info['attributes']['expires']\n",
    "        status = \"ok\"\n",
    "        if expires:\n",
    "            expires = datetime.datetime.strptime(expires.split('T')[0], '%Y-%m-%d')\n",
    "            expiry_delta = expires - created\n",
    "            if today > expires:\n",
    "                satus = \"expired\"\n",
    "            elif expiry_delta > datetime.timedelta(days=MAX_EXPIRY_ROTATION_DAYS):\n",
    "                status = \"exceeds max expiry days\"\n",
    "            # convert times back to a string for display\n",
    "            expires = expires.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            status = \"no expiry\"\n",
    "            expires = \"None\"\n",
    "        created = created.strftime('%Y-%m-%d')            \n",
    "            \n",
    "        return status, created, expires\n",
    "    \n",
    "    for keyvault_name, metadata in keyvault_keys_and_secrets_metadata.items():\n",
    "        for key_info in metadata['keys']:\n",
    "            items_checked += 1\n",
    "            key_name = key_info['kid'].split('/')[-1]\n",
    "            status, created, expires = get_key_or_secret_status(key_info)\n",
    "            if status != \"ok\":\n",
    "                items_flagged_list.append((keyvault_name, key_name, \"key\", status, created, expires))\n",
    "                \n",
    "        for secret_info in metadata['secrets']:\n",
    "            items_checked += 1\n",
    "            secret_name = secret_info['id'].split('/')[-1]\n",
    "            status, created, expires = get_key_or_secret_status(secret_info)                \n",
    "            if status != \"ok\":\n",
    "                items_flagged_list.append((keyvault_name, secret_name, \"secret\", status, created, expires))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': items_checked}\n",
    "    metadata = {\"finding_name\": \"expiry_date_is_set_on_all_keys_and_secrets\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"KeyVault Name\", \"Name\", \"Type\", \"Status\", \"Created\", \"Expires\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "critical_resources_list = []\n",
    "def resource_locks_are_set_for_mission_critical_azure_resources_8_3(critical_resources_list, locked_resources):\n",
    "    items_flagged_list = []\n",
    "    if len(locked_resources) == 0 and len(critical_resources_list) == 0:\n",
    "        stats = {'items_flagged': 1,\n",
    "                 'items_checked': 1}\n",
    "    else:\n",
    "        {'items_flagged': len(critical_resources_list) - len(locked_resources),\n",
    "         'items_checked': len(critical_resources_list)}\n",
    "        items_flagged_list = critical_resources_list.intersection(locked_resources)\n",
    "    metadata = {\"finding_name\": \"resource_locks_are_set_for_mission_critical_Azure_resources\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def test_controls():\n",
    "    keyvault_keys_and_secrets_metadata = load_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path)\n",
    "    locked_resources = load_locked_resources(locked_resources_path)\n",
    "    results = {}\n",
    "    results['expiry_date_is_set_on_all_keys_and_secrets'] = expiry_date_is_set_on_all_keys_and_secrets(keyvault_keys_and_secrets_metadata)\n",
    "    results['resource_locks_are_set_for_mission_critical_azure_resources'] = resource_locks_are_set_for_mission_critical_azure_resources_8_3(critical_resources_list, locked_resources)\n",
    "    \n",
    "    with open(other_security_considerations_filtered_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4, sort_keys=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvaults = load_keyvaults(keyvaults_path)\n",
    "keyvault_keys_and_secrets_metadata = get_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path, keyvaults)\n",
    "keyvault_keys_and_secrets_metadata = load_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path)\n",
    "keyvault_keys_and_secrets_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile render_utils.py\n",
    "import functools\n",
    "import yaml\n",
    "\n",
    "cis_scanner_root = '/praetorian-tools/azure_cis_scanner/'\n",
    "scans_base = os.path.expanduser('/engagements/tcb-edp/scans')\n",
    "\n",
    "@functools.lru_cache(1, typed=False)\n",
    "def get_dirs(directory):\n",
    "    return [x for x in os.listdir(directory) if os.path.isdir(directory)]\n",
    "\n",
    "# figure out better way to get base dir or let user select in UI\n",
    "active_subscription_dir = get_dirs(scans_base)[0]\n",
    "active_subscription_dir = subscription_dirname\n",
    "#active_subscription_dir = 'Development-6ff7f744'\n",
    "\n",
    "accounts = {}\n",
    "with open(os.path.join(scans_base, 'accounts.json'), 'r') as f:\n",
    "    accounts = yaml.load(f)\n",
    "\n",
    "scans_root = os.path.join(scans_base, active_subscription_dir)\n",
    "\n",
    "#APP_ROOT = os.path.dirname(os.path.abspath(__file__))\n",
    "APP_ROOT = os.path.join(cis_scanner_root, 'report')\n",
    "STATIC = os.path.join(APP_ROOT, 'static')\n",
    "\n",
    "with open(os.path.expanduser(APP_ROOT + '/cis_structure.yaml'), 'r') as f:\n",
    "    cis_structure = yaml.load(f)\n",
    "\n",
    "def set_scans_root(subscription_dirname=active_subscription_dir):\n",
    "    scans_root = os.path.join(scans_base, subscription_dirname)\n",
    "    return scans_root\n",
    "\n",
    "@functools.lru_cache(maxsize=32, typed=False)\n",
    "def get_filtered_data_path(date=None, subscription_dirname=active_subscription_dir):\n",
    "    \"\"\"\n",
    "    Get the filtered data root for the scan run on date=date or latest if date=None\n",
    "    Returns path, date where date is the most recent date with data <= requested date\n",
    "    \n",
    "    Directory structure is\n",
    "    <scans_root>/scans/<date>/<section_lowercase_underscores>.json\n",
    "    \"\"\"\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    if date:\n",
    "        if os.path.exists:\n",
    "            return os.path.join(scans_root, 'scans', date, 'filtered'), date\n",
    "        else:\n",
    "            raise ValueError(\"Filtered data requested for {} but file does not exist at {}\".format(\n",
    "                date, os.path.join(scans_root, 'scans')))\n",
    "    else:\n",
    "        dir_list = get_dirs(scans_root)\n",
    "        if len(dir_list) == 0:\n",
    "            print(\"No data found in {}.  Please run scanner first\".format(scans_root))\n",
    "        else:\n",
    "            date = sorted(dir_list)[0]\n",
    "            return os.path.join(scans_root, 'scans', date, 'filtered'), date\n",
    "\n",
    "@functools.lru_cache(maxsize=32, typed=False)\n",
    "def get_filtered_data(date=None, subscription_dirname=active_subscription_dir):\n",
    "    \"\"\"\n",
    "    Returns a dict of filtered data for a specific date or latest (default)\n",
    "    \n",
    "    If a section is missing it will not be returned.\n",
    "    The structure is {\"Identity and Access Management\": {\"finding1\": results_dict1}, \"finding2\": results_dict2}\n",
    "    where results_dict has keys stats, metadata, items, date - where date is actual date where data was found\n",
    "    \"\"\"\n",
    "    subscription_dirname = set_scans_root(subscription_dirname)\n",
    "    filtered_data = {}\n",
    "    for section_name in cis_structure['section_ordering']:\n",
    "        section_data = get_filtered_data_by_section(section_name, subscription_dirname=subscription_dirname)\n",
    "        filtered_data[section_name] = section_data\n",
    "    return filtered_data\n",
    "\n",
    "@functools.lru_cache(maxsize=128, typed=False)\n",
    "def get_filtered_data_by_section(section_name, date=None, subscription_dirname=active_subscription_dir):\n",
    "    \"\"\"\n",
    "    Get the latest data for a section returning first found <= date\n",
    "    @params sectoin_name: Name of CIS section as a string e.g. (\"Identity and Access Management\")\n",
    "    @params date: date in format 'YYYY-M-D', i.e. strftime(\"%Y-%m-%d\")\n",
    "    @returns filtered data, date\n",
    "    \"\"\"\n",
    "    # get date folders, most to least recent\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    dir_list = reversed(sorted(get_dirs(scans_root)))\n",
    "    section_name_file = '_'.join(map(str.lower, section_name.split(' '))) + '_filtered.json'\n",
    "    for dir_date in dir_list:\n",
    "        if date and (dir_date > date):\n",
    "            continue\n",
    "        filtered_data_path = os.path.join(scans_root, dir_date, 'filtered', section_name_file)\n",
    "        if os.path.exists(filtered_data_path):\n",
    "            with open(filtered_data_path, 'r') as f:\n",
    "                data = yaml.load(f)\n",
    "                data['date'] = dir_date\n",
    "                return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=1, typed=False)\n",
    "def get_latest_filtered_data(date=None, subscription_dirname=active_subscription_dir):\n",
    "    \"\"\"\n",
    "    Returns a dict as in get_filtered_data, but if a section is missing, it will search\n",
    "    back in time for a date where the section does exist.\n",
    "    \"\"\"\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    data = get_filtered_data(date)\n",
    "    if not data:\n",
    "        return None\n",
    "    else:\n",
    "        for section_name in cis_structure['section_ordering']:\n",
    "            if section_name not in data:\n",
    "                section_data = get_filtered_data_by_name(section_name, date, subscription_dirname=subsription_dirname)\n",
    "                if section_data:\n",
    "                    data[section_name] = section_data\n",
    "    return data\n",
    "\n",
    "@functools.lru_cache(maxsize=1, typed=False)\n",
    "def get_stats(subscription_dirname=active_subscription_dir):\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    stats = {}\n",
    "    dir_list = sorted(get_dirs(scans_root))\n",
    "    for section_name in cis_structure['section_ordering']:\n",
    "        stats[section_name] = {}\n",
    "        section_name_file = '_'.join(map(str.lower, section_name.split(' '))) + '_filtered.json'\n",
    "        for dir_date in dir_list:\n",
    "            filtered_data_path = os.path.join(scans_root, dir_date, 'filtered', section_name_file)\n",
    "            if os.path.exists(filtered_data_path):\n",
    "                with open(filtered_data_path, 'r') as f:\n",
    "                    data = yaml.load(f)\n",
    "                for finding_name, finding_data in data.items():\n",
    "                    if not finding_name in stats[section_name]:\n",
    "                        stats[section_name][finding_name] = {}\n",
    "                    stats[section_name][finding_name][dir_date] = finding_data['stats']\n",
    "    return stats\n",
    "\n",
    "@functools.lru_cache(maxsize=1, typed=False)\n",
    "def get_latest_stats(subscription_dirname=active_subscription_dir):\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    latest_stats = {}\n",
    "    stats = get_stats()\n",
    "    for section_name in stats:\n",
    "        latest_stats[section_name] = {}\n",
    "        for finding_name in stats[section_name]:\n",
    "            date = max(stats[section_name][finding_name])\n",
    "            latest_stats[section_name][finding_name] = {\"date\": date, **stats[section_name][finding_name][date]}\n",
    "\n",
    "    return latest_stats\n",
    "\n",
    "def get_finding_name(finding_name, subsection_name):\n",
    "    \"\"\"\n",
    "    Get finding name from CIS_TOC using over-ride (finding_name) but defaulting to parsed subsection_name\n",
    "    \"\"\"\n",
    "    if finding_name:\n",
    "        return finding_name\n",
    "    else:\n",
    "        return underscore_name(subsection_name)\n",
    "\n",
    "def underscore_name(subsection_name):\n",
    "    return '_'.join(map(lambda x: x.lower(), subsection_name.split(' ')))\n",
    "\n",
    "def title_except(string):\n",
    "    articles = ['a', 'an', 'of', 'the', 'is', 'not', 'for', 'if']\n",
    "    word_list = re.split(' ', string)       # re.split behaves as expected\n",
    "    final = [word_list[0].capitalize()]\n",
    "    for word in word_list[1:]:\n",
    "        final.append(word if word in articles else word.capitalize())\n",
    "    return \" \".join(final)\n",
    "\n",
    "def get_finding_index(findings_list, finding):\n",
    "    for finding_entry in findings_list:\n",
    "        if finding_entry['subsection_name'] == finding:\n",
    "            return finding_entry\n",
    "    raise ValueError(\"finding {} not found in {}\".format(finding, findings_list))\n",
    "    \n",
    "# def get_summary_stats(section=None):\n",
    "# \t\"\"\"\n",
    "# \tReturn the sum over impacted_items in all findings in a section\n",
    "\n",
    "# \tIf section == None, sum over sections\n",
    "# \t\"\"\"\n",
    "# \tstats = get_stats()\n",
    "# \tif section:\n",
    "\n",
    "# \telse:\n",
    "# \t\tfor section in stats:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def find_finding_entry(section_toc, finding_underscore_name):\n",
    "    for finding_entry in section_toc:\n",
    "        if finding_underscore_name == get_finding_name(finding_entry['finding_name'], finding_entry['subsection_name']):\n",
    "            return finding_entry\n",
    "    return None\n",
    "\n",
    "def findings_summary(latex=False, subscription_dirname=subscription_dirname):\n",
    "    print('1', subscription_dirname)\n",
    "    data = get_filtered_data(subscription_dirname=subscription_dirname)\n",
    "    for section_name, section_findings in data.items():\n",
    "        if section_findings:\n",
    "            for finding_name, finding in section_findings.items():\n",
    "                if finding_name == 'date':\n",
    "                    continue\n",
    "                try:\n",
    "                    section_toc = cis_structure['TOC'][section_name]\n",
    "                    finding_entry = find_finding_entry(section_toc, finding_name)\n",
    "                    if finding_entry:\n",
    "                        title = finding_entry['subsection_name']\n",
    "                    else:\n",
    "                        title = ' '.join(finding_name.split('_'))\n",
    "                    if finding.get('stats') and (finding['stats']['items_flagged'] > 0):\n",
    "                        print(\"{}: {} - {} of {} failed\".format(section_name, finding_name, finding['stats']['items_flagged'], finding['stats']['items_checked']))\n",
    "                        if latex:\n",
    "                            render_latex(finding['items'], finding['metadata']['columns'], title_except(title))\n",
    "                        else:\n",
    "                            pprint(finding['items'])\n",
    "                        print('\\n')\n",
    "                    if finding_name in ['security_contact_phone_number_is_set', 'activity_log_alert_is_configured']:\n",
    "                        print(finding['items'])\n",
    "                except Exception as e:\n",
    "                    print(\"           finding\", finding.keys())\n",
    "                    print(e)\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings_summary(latex=True, subscription_dirname=subscription_dirname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings_summary(latex=True, subscription_dirname='Development-6ff7f744')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import yaml\n",
    "    \n",
    "def clean_latex(tuple_entry):\n",
    "    \"\"\"\n",
    "    Filter/escape problematic characters\n",
    "    Our minerva pdf generator chokes on '_', '*', ...\n",
    "    and possibly other things.\n",
    "    \"\"\"\n",
    "    def _clean_latex(tuple_entry_string):\n",
    "        processed = False\n",
    "        for symbol in ['_', '*']:\n",
    "            if symbol in tuple_entry_string:\n",
    "                tuple_entry_string = tuple_entry_string.replace(symbol, '\\\\' + symbol)\n",
    "                processed = True\n",
    "        if processed:\n",
    "            return '\\\\texttt{' + tuple_entry_string + '}'\n",
    "        else:\n",
    "            return tuple_entry_string\n",
    "\n",
    "    return _clean_latex(str(tuple_entry))\n",
    "    \n",
    "    \n",
    "def render_latex(resource_tuples, header, title):\n",
    "    header = list(map(clean_latex, header))\n",
    "    title = clean_latex(title)\n",
    "    \n",
    "    render_table_start(header, title)\n",
    "    num_columns = len(header)\n",
    "    if num_columns > 1:\n",
    "        line = ' & '.join(['{}']*num_columns)\n",
    "    else:\n",
    "        line = '{}'\n",
    "    line = line + ' \\\\\\\\'\n",
    "    for resource_tuple in resource_tuples:\n",
    "        if type(resource_tuple) == str:\n",
    "            resource_tuple = (resource_tuple,)\n",
    "        print('    ' + line.format(*map(clean_latex, resource_tuple)))\n",
    "    render_table_end(header)\n",
    "    \n",
    "def render_table_start(header, title):\n",
    "    \"\"\"\n",
    "    Render latex table suitable for minerva rendering\n",
    "    \n",
    "            \n",
    "    If the elements in the table are very long you can correct spacing with invisible text like this:\n",
    "    {\\color[HTML]{FFFFFF} {}} & {\\color[HTML]{333333}{spacing}}{\\color[HTML]{FFFFFF} {}}{\\color[HTML]{333333}{spacing_2}} & {\\color[HTML]{333333}{spacing_3}}{\\color[HTML]{FFFFFF}{}} \\\\\n",
    "\n",
    "    TODO: Paginate pages to break nicely over many pages.\n",
    "    \n",
    "    Some exceptions used to manually fix for specific cases:\n",
    "    \n",
    "    Text wrap some long arrays based on answer by zyy on\n",
    "    https://tex.stackexchange.com/questions/54069/table-with-text-wrapping\n",
    "    \n",
    "    GROUPS column had variable length and this worked.\n",
    "    \\begin{tabular}{|l|>{\\centering\\arraybackslash}m{10cm}|}\n",
    "    \\hline\n",
    "    \\multicolumn{2}{|c|}{IAM Users without MFA} \\\\\n",
    "    \\rowcolor[HTML]{333333}\n",
    "    {\\color[HTML]{FFFFFF}USER NAME} & {\\color[HTML]{FFFFFF}GROUPS} \\\\\n",
    "    \"\"\"\n",
    "    num_columns = len(header)\n",
    "    entries = ['\\color[HTML]{FFFFFF}' + '{}'.format(clean_latex(x)) for x in header]\n",
    "    if num_columns > 1: \n",
    "        line = '} & {'.join(entries)\n",
    "    else:\n",
    "        line = entries[0]\n",
    "    columns_format = '{|' + '|'.join(['l']*num_columns) + '|}'\n",
    "    print('\\\\begin{tabular}' + '{}'.format(columns_format) + '\\n'\n",
    "         '    ' + '\\\\hline\\n' +\n",
    "         '    ' + '\\\\multicolumn{' + str(num_columns) + '}' +\n",
    "        '{|c|}' + '{' + title + '}' +  ' \\\\\\\\\\n' +\n",
    "        '    ' + '\\\\rowcolor[HTML]{333333}\\n' +\n",
    "        '    ' + '{' + line + '}' + ' \\\\\\\\' \n",
    "    )\n",
    "    \n",
    "\n",
    "def render_table_end(header):\n",
    "    \"\"\"\n",
    "    Render table end\n",
    "    \"\"\"\n",
    "    print('    \\\\hline\\n' +\n",
    "          '\\\\end{tabular}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az monitor diagnostic-settings list --resource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "header = [\"KeyVault\", \"Name\", \"Type\", \"Status\", \"Created\", \"Expires\"]\n",
    "render_latex(expiry_dates, header, \"Expiry Date is Set on All Keys and Secrets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sql_results.keys():\n",
    "    print(key.split('_')[-1], key, len(sql_results[key]))\n",
    "    for resource_group, server_name, db_name in sql_results[key]:\n",
    "        print('{resource_group} & {server_name} & {db_name} \\\\\\\\'.format(resource_group=resource_group,\n",
    "             server_name=server_name,\n",
    "             db_name=db_name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_results(results, impacted_system_dict):\n",
    "    \"\"\"\n",
    "    Wraps functions that return a tuple (passed(bool), impacted_closure(OrderedDict), result(json object - dict, list, ...)\n",
    "    @results: object reperesenting results dict of {func.__name__'s: [(impacted_system_dict + impacted_closure, result)]\n",
    "              impacted_system_tyuple is know prior to query\n",
    "              impacted_closure is the final identifying component(s) such as {db: db1} when (resource_group, sql_server) are given\n",
    "                  $ az sql db list --resource-group $resource_group --server $server_name\n",
    "\n",
    "    @impacted_systems_dict: OrderedDict([(key1, val1), (key2, val2),]) identifying (possibly) failing resource\n",
    "                 eg. {resource_group: rg-val, server: server-val} for sql tests\n",
    "                 or {resource_group: rg-val, storage_account: storage-val, disk: disk-val) for storage\n",
    "                 impacted_system_tuple is often required in the query:\n",
    "    @returns: (bool) passed, (OrderedDict) impacted_system, (json object) result\n",
    "              where impacted_system = OrderedDict(list(impacted_system_dict.items()) + list(impacted_closure.items())) \n",
    "    \"\"\"\n",
    "    def decorate(func):\n",
    "        def call(*args, **kwargs):\n",
    "            # *impacted_closure matches anything between first and last in func's return tuple if it exists\n",
    "            passed, *impacted_closure, result = func(*args, **kwargs)\n",
    "            impacted_system = OrderedDict(list(impacted_system_dict.items()) + list(impacted_closure.items()))\n",
    "            return apply_gen_findings(func, passed, impacted_system, result, results, *args, **kwargs)\n",
    "        return call\n",
    "    return decorate\n",
    "\n",
    "def apply_gen_results(func, passed, impacted_system, result, results, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    @func: function to wrap\n",
    "    @passed: bool - True of passed, False if failed\n",
    "    @impacted_system: OrderedDict(list(impacted_system_dict.items()) + list(impacted_closure.items())) \n",
    "    @result: object with important output data from test\n",
    "    @results: dict with keys func.__name__ and values list of dicts with keys impacted_system_tuple, value data\n",
    "    \"\"\"\n",
    "    if not passed:\n",
    "        pairs = results.get(func.__name__, [])\n",
    "        pairs.append((impacted_system, result))\n",
    "        results[func.__name__] = pairs\n",
    "    return passed, impacted_system, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(\"expiry_date_is_set_on_all_keys_and_secrets\".split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
